{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce46b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.read_edgelist('./dataset/twitch/DE/musae_DE_edges.csv', delimiter=',', nodetype=int, data=(('weight', float),))\n",
    "G = nx.Graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "144ad785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 9498 nodes and 153138 edges\n"
     ]
    }
   ],
   "source": [
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd5766e",
   "metadata": {},
   "source": [
    "## Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41971dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating ground truth values...\n",
      "  Connectivity: True (in 0.0207s)\n",
      "  MST Weight (or Forest): 9497.00 (9497 edges) (in 0.3544s)\n",
      "  Calculated 9498 node degrees (in 0.0029s)\n",
      "Ground truth calculation complete.\n",
      "\n",
      "--- Running Comparisons on Graph (N=9498, M=153138) ---\n",
      "\n",
      "Algorithm: GraphConnectivitySketch\n",
      "  Build Time: 0.6014s\n",
      "  Query Time (is_connected avg 5 runs): 0.009326s\n",
      "  Memory Estimate: 117606.71 KB\n",
      "  Is Connected (Sketch): False (vs Ground Truth: True) -> Accuracy: False\n",
      "\n",
      "Algorithm: GraphLinearMeasurements\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1775\u001b[39m\n\u001b[32m   1772\u001b[39m ground_truth = get_ground_truth(G)\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# --- Run Algorithm Comparison ---\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m comparison_results, _ = \u001b[43mrun_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[38;5;66;03m# --- Print Summary Table ---\u001b[39;00m\n\u001b[32m   1778\u001b[39m results_df = pd.DataFrame(comparison_results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1362\u001b[39m, in \u001b[36mrun_comparison\u001b[39m\u001b[34m(G, ground_truth)\u001b[39m\n\u001b[32m   1360\u001b[39m linear_sketch = GraphLinearMeasurements(n, c=c_param)\n\u001b[32m   1361\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m u, v \u001b[38;5;129;01min\u001b[39;00m G.edges():\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m     \u001b[43mlinear_sketch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Assuming unweighted edges for simplicity\u001b[39;00m\n\u001b[32m   1363\u001b[39m build_time = time.time() - start_build\n\u001b[32m   1364\u001b[39m mem_estimate = linear_sketch.get_memory_estimate()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 291\u001b[39m, in \u001b[36mGraphLinearMeasurements.update\u001b[39m\u001b[34m(self, u, v, delta)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;66;03m# Skip if self-loop or invalid edge\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.m):\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# Accessing signs[i][e] will create the entry with a random sign if it doesn't exist\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     sign_i_e = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msigns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.y[i] += delta * sign_i_e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 269\u001b[39m, in \u001b[36mGraphLinearMeasurements.__init__.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28mself\u001b[39m.m = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mint\u001b[39m(c * n * log_n_sq))\n\u001b[32m    267\u001b[39m \u001b[38;5;66;03m# Using defaultdict for signs simplifies adding new edges encountered\u001b[39;00m\n\u001b[32m    268\u001b[39m \u001b[38;5;66;03m# Default factory assigns a random sign {+1, -1} to unseen edges\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m \u001b[38;5;28mself\u001b[39m.signs = [defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.m)]\n\u001b[32m    270\u001b[39m \u001b[38;5;28mself\u001b[39m.y = [\u001b[32m0.0\u001b[39m] * \u001b[38;5;28mself\u001b[39m.m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py3.12/lib/python3.12/random.py:348\u001b[39m, in \u001b[36mRandom.choice\u001b[39m\u001b[34m(self, seq)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(seq):\n\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mCannot choose from an empty sequence\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m seq[\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_randbelow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py3.12/lib/python3.12/random.py:247\u001b[39m, in \u001b[36mRandom._randbelow_with_getrandbits\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    245\u001b[39m getrandbits = \u001b[38;5;28mself\u001b[39m.getrandbits\n\u001b[32m    246\u001b[39m k = n.bit_length()\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m r = \u001b[43mgetrandbits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 0 <= r < 2**k\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m r >= n:\n\u001b[32m    249\u001b[39m     r = getrandbits(k)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- START OF (MODIFIED) FILE project1.0.py ---\n",
    "\n",
    "# Original classes from project1.0.py go here...\n",
    "# (L0Sampler, GraphConnectivitySketch, GraphLinearMeasurements, AdaptiveGraphSketch,\n",
    "#  ModifiedBoruvkaSketching, SimpleSetSketch, CountMinSketch, gSketch,\n",
    "#  GraphVisualSketchHierarchy)\n",
    "# ...\n",
    "# I will paste the original classes here for completeness, with minor fixes/imports.\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import hashlib # Added for CountMinSketch, gSketch\n",
    "from collections import defaultdict\n",
    "import heapq # Corrected from 'heap' import\n",
    "\n",
    "# --- L0Sampler Class ---\n",
    "class L0Sampler:\n",
    "    \"\"\"\n",
    "    A very simple 'L0-sampler' over a dynamic multiset of items:\n",
    "      - supports insert(item) and delete(item)\n",
    "      - sample() returns a uniformly random item among those currently present (or None)\n",
    "    Note: deletion implementation here swaps with the end.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "        self.pos = defaultdict(set)\n",
    "\n",
    "    def insert(self, item):\n",
    "        i = len(self.items)\n",
    "        self.items.append(item)\n",
    "        self.pos[item].add(i)\n",
    "\n",
    "    def delete(self, item):\n",
    "        if not self.pos[item]:\n",
    "            # Item not present or already removed its last instance\n",
    "            # Check if it exists anywhere else in pos, indicating multiple copies were inserted\n",
    "            # and we are trying to delete one that doesn't match the last known position.\n",
    "            # This basic implementation doesn't handle multiset counts perfectly on deletion.\n",
    "            # For robust multiset deletion, counts or a different structure would be needed.\n",
    "             return # Or raise an error if deletion of non-existent is critical\n",
    "\n",
    "        try:\n",
    "            i = self.pos[item].pop() # Get one position of the item\n",
    "        except KeyError:\n",
    "             # This can happen if the item was already fully removed via other means\n",
    "             # or if defaultdict logic has issues.\n",
    "             return\n",
    "\n",
    "        if i >= len(self.items):\n",
    "             # This indicates an inconsistent state, potentially due to prior errors\n",
    "             # or complex interactions not fully handled by this simple structure.\n",
    "             # Silently return or log an error.\n",
    "             # Let's try to ensure the set is clean for the item if its position is invalid.\n",
    "             if item in self.pos and not self.pos[item]:\n",
    "                 del self.pos[item]\n",
    "             return\n",
    "\n",
    "\n",
    "        # If 'i' is the last element's position, just pop\n",
    "        if i == len(self.items) - 1:\n",
    "            self.items.pop()\n",
    "            # If the set for 'item' becomes empty after popping 'i', remove the item key\n",
    "            if item in self.pos and not self.pos[item]:\n",
    "                 del self.pos[item]\n",
    "            return\n",
    "\n",
    "        # Otherwise, swap with the last element\n",
    "        last_item = self.items.pop() # Remove last element first\n",
    "        last_item_original_pos = len(self.items) # Its original position was the new length\n",
    "\n",
    "        self.items[i] = last_item # Place last_item into the vacated spot 'i'\n",
    "\n",
    "        # Update position tracking for the moved last_item\n",
    "        if last_item in self.pos:\n",
    "             # Remove its old position (end of the list)\n",
    "             self.pos[last_item].discard(last_item_original_pos)\n",
    "             # Add its new position 'i'\n",
    "             self.pos[last_item].add(i)\n",
    "\n",
    "        # Clean up the entry for the deleted item if its position set is now empty\n",
    "        if item in self.pos and not self.pos[item]:\n",
    "            del self.pos[item]\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Return a random item, or None if empty.\"\"\"\n",
    "        if not self.items:\n",
    "            return None\n",
    "        # Ensure sampling only happens if items list is not empty\n",
    "        try:\n",
    "            return random.choice(self.items)\n",
    "        except IndexError:\n",
    "            # This could happen if items becomes empty between the check and choice\n",
    "            # due to concurrency, though unlikely in this single-threaded context.\n",
    "            # Or if self.items somehow became non-list temporarily (unlikely).\n",
    "            return None\n",
    "\n",
    "    def get_memory_estimate(self):\n",
    "        # Basic memory estimation\n",
    "        items_mem = sys.getsizeof(self.items) + sum(sys.getsizeof(x) for x in self.items)\n",
    "        pos_mem = sys.getsizeof(self.pos)\n",
    "        for k, v in self.pos.items():\n",
    "            pos_mem += sys.getsizeof(k) + sys.getsizeof(v) + sum(sys.getsizeof(x) for x in v)\n",
    "        return items_mem + pos_mem\n",
    "\n",
    "# --- GraphConnectivitySketch Class ---\n",
    "class GraphConnectivitySketch:\n",
    "    \"\"\"\n",
    "    Maintains a dynamic undirected graph on n nodes via one L0Sampler per node.\n",
    "    Supports edge insertions/deletions in O(1) time (amortized),\n",
    "    and can extract a spanning forest in O(n) time by sampling each node once.\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        # Ensure n is non-negative\n",
    "        if n < 0:\n",
    "            raise ValueError(\"Number of nodes 'n' cannot be negative.\")\n",
    "        self.samplers = [L0Sampler() for _ in range(n)]\n",
    "        self.edges = set() # Track edges to avoid double adding/deleting effects\n",
    "\n",
    "    def _encode_edge(self, u, v):\n",
    "        \"\"\"Encode an undirected edge as an ordered pair for deduplication.\"\"\"\n",
    "        # Add boundary checks\n",
    "        if not (0 <= u < self.n and 0 <= v < self.n):\n",
    "             raise ValueError(f\"Vertices {u}, {v} out of range [0, {self.n-1}]\")\n",
    "        if u == v:\n",
    "             # Decide how to handle self-loops. Often ignored in connectivity.\n",
    "             return None # Or raise an error if needed\n",
    "        return tuple(sorted((u, v)))\n",
    "\n",
    "    def add_edge(self, u, v):\n",
    "        e = self._encode_edge(u, v)\n",
    "        if e is None or e in self.edges: # Handle self-loops or existing edges\n",
    "            return\n",
    "        self.edges.add(e)\n",
    "\n",
    "        # Check bounds before accessing samplers\n",
    "        if 0 <= u < self.n and 0 <= v < self.n:\n",
    "             self.samplers[u].insert(v)\n",
    "             self.samplers[v].insert(u)\n",
    "        else:\n",
    "             # This case should ideally be caught by _encode_edge, but double-check.\n",
    "             print(f\"Warning: Attempted to add edge ({u}, {v}) with out-of-bounds node.\")\n",
    "\n",
    "    def del_edge(self, u, v):\n",
    "        e = self._encode_edge(u, v)\n",
    "        if e is None or e not in self.edges:\n",
    "            return\n",
    "        self.edges.remove(e)\n",
    "        # Check bounds before accessing samplers\n",
    "        if 0 <= u < self.n and 0 <= v < self.n:\n",
    "             self.samplers[u].delete(v)\n",
    "             self.samplers[v].delete(u)\n",
    "        else:\n",
    "             print(f\"Warning: Attempted to delete edge ({u}, {v}) with out-of-bounds node.\")\n",
    "\n",
    "\n",
    "    def spanning_forest(self):\n",
    "        \"\"\"\n",
    "        Extract a spanning forest using one L0 sample per node.\n",
    "        This is like one round of the Ahn–Guha–McGregor algorithm:\n",
    "        for each node u, sample a random neighbour v; if u and v are in\n",
    "        different tree‐components, add (u,v) to the forest.\n",
    "        Uses Union-Find data structure for efficiency.\n",
    "        \"\"\"\n",
    "        if self.n == 0: return [] # Handle empty graph case\n",
    "\n",
    "        parent = list(range(self.n))\n",
    "        num_edges = 0\n",
    "        # Path compression find\n",
    "        def find(i):\n",
    "            if parent[i] == i:\n",
    "                return i\n",
    "            parent[i] = find(parent[i]) # Path compression\n",
    "            return parent[i]\n",
    "\n",
    "        # Union by rank/size is not strictly needed here, basic union works\n",
    "        def union(i, j):\n",
    "            nonlocal num_edges\n",
    "            root_i = find(i)\n",
    "            root_j = find(j)\n",
    "            if root_i != root_j:\n",
    "                parent[root_j] = root_i # Simple union\n",
    "                # Returning True indicates a successful merge that added an edge to the conceptual forest\n",
    "                return True\n",
    "            return False # Nodes were already in the same component\n",
    "\n",
    "        forest_edges = []\n",
    "        for u in range(self.n):\n",
    "            if u >= len(self.samplers): # Safety check\n",
    "                 print(f\"Warning: Node index {u} out of bounds for samplers.\")\n",
    "                 continue\n",
    "            v = self.samplers[u].sample()\n",
    "            if v is None:\n",
    "                continue\n",
    "            # Ensure sampled neighbor v is within valid node range\n",
    "            if not (0 <= v < self.n):\n",
    "                print(f\"Warning: Sampler for node {u} returned invalid neighbor {v}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            if union(u, v):\n",
    "                forest_edges.append(tuple(sorted((u, v)))) # Store edges consistently\n",
    "                num_edges += 1\n",
    "\n",
    "\n",
    "        return forest_edges\n",
    "\n",
    "\n",
    "    def is_connected(self):\n",
    "        \"\"\"\n",
    "        Check if the whole graph is likely connected by seeing if the extracted\n",
    "        spanning forest has exactly n-1 edges.\n",
    "        Requires n >= 1. Note: This is probabilistic, not guaranteed deterministic.\n",
    "        \"\"\"\n",
    "        if self.n <= 0: return False # Or True if n=0 is considered connected\n",
    "        if self.n == 1: return True\n",
    "\n",
    "        # The AGM single-round approach doesn't guarantee finding a full spanning\n",
    "        # tree even if one exists. It finds a spanning forest. Connectivity check\n",
    "        # based *solely* on len(forest) == n-1 from *one round* is heuristic and\n",
    "        # might give false negatives.\n",
    "        # A more robust check would involve running the forest finding multiple times\n",
    "        # or using the full Union-Find state after processing all samples.\n",
    "\n",
    "        # We run the forest extraction and check the number of edges.\n",
    "        # This matches the original code's logic but inherits its probabilistic nature.\n",
    "        forest = self.spanning_forest()\n",
    "        return len(forest) == self.n - 1\n",
    "\n",
    "        # Alternative (more deterministic check using the final state of union-find):\n",
    "        # Run spanning_forest to populate the parent array via union operations.\n",
    "        # Then, count the number of distinct roots remaining.\n",
    "        # parent = list(range(self.n)) # Need to re-init or use the one from spanning_forest\n",
    "        # def find(i): ... # Define find again or pass parent array around\n",
    "        # roots = {find(i) for i in range(self.n)}\n",
    "        # return len(roots) == 1 if self.n > 0 else True\n",
    "\n",
    "\n",
    "    def get_memory_estimate(self):\n",
    "        base_mem = sys.getsizeof(self.n) + sys.getsizeof(self.edges) + sys.getsizeof(self.samplers)\n",
    "        samplers_mem = sum(s.get_memory_estimate() for s in self.samplers)\n",
    "        edges_mem = sum(sys.getsizeof(e) for e in self.edges)\n",
    "        return base_mem + samplers_mem + edges_mem\n",
    "\n",
    "# --- GraphLinearMeasurements Class ---\n",
    "class GraphLinearMeasurements:\n",
    "    \"\"\"\n",
    "    Uses m = O(n · polylog n) random linear measurements of the incidence stream.\n",
    "    Supports edge insertions/deletions in O(m) time per update.\n",
    "    NOTE: Decoding routines (connectivity, MST etc.) are placeholders.\n",
    "    \"\"\"\n",
    "    def __init__(self, n, c=4):\n",
    "        if n < 0: raise ValueError(\"Number of vertices n cannot be negative.\")\n",
    "        self.n = n\n",
    "\n",
    "        if n <= 1:\n",
    "             log_n_sq = 1 # Avoid log(0) or log(1) issues\n",
    "        else:\n",
    "             # Use max(1, ...) to ensure log_n_sq is at least 1 if n is small but > 1\n",
    "             log_n_sq = max(1, math.log(n, 2)) ** 2\n",
    "\n",
    "        # Ensure m is at least 1\n",
    "        self.m = max(1, int(c * n * log_n_sq))\n",
    "\n",
    "        # Using defaultdict for signs simplifies adding new edges encountered\n",
    "        # Default factory assigns a random sign {+1, -1} to unseen edges\n",
    "        self.signs = [defaultdict(lambda: random.choice([1, -1])) for _ in range(self.m)]\n",
    "        self.y = [0.0] * self.m # The sketch vector\n",
    "\n",
    "    def _encode_edge(self, u, v):\n",
    "        if not (0 <= u < self.n and 0 <= v < self.n):\n",
    "            raise ValueError(f\"Vertices {u}, {v} out of range [0, {self.n-1}]\")\n",
    "        if u == v:\n",
    "            # Standard linear sketches often ignore self-loops or handle them specifically.\n",
    "            # Raise error or return None to signal it shouldn't be processed.\n",
    "            # raise ValueError(\"Self-loops are not typically handled in this basic model.\")\n",
    "             return None # Silently ignore self-loops\n",
    "        return tuple(sorted((u, v)))\n",
    "\n",
    "    def update(self, u, v, delta=1):\n",
    "        \"\"\"Process edge update (u, v) with change delta.\"\"\"\n",
    "        if delta == 0: return\n",
    "\n",
    "        e = self._encode_edge(u, v)\n",
    "        if e is None: return # Skip if self-loop or invalid edge\n",
    "\n",
    "        for i in range(self.m):\n",
    "            # Accessing signs[i][e] will create the entry with a random sign if it doesn't exist\n",
    "            sign_i_e = self.signs[i][e]\n",
    "            self.y[i] += delta * sign_i_e\n",
    "\n",
    "\n",
    "    # --- Placeholder Decoding Functions ---\n",
    "    # These require complex algorithms not implemented here.\n",
    "    # They will return dummy values for the benchmark.\n",
    "\n",
    "    def _placeholder_decode_sketch(self):\n",
    "        \"\"\"Placeholder: Real decoding is complex.\"\"\"\n",
    "        print(\"[WARN] Using placeholder decode_sketch. Returning empty candidate set.\")\n",
    "        # Simulate some work based on sketch size 'm' for timing? No, keep it fast.\n",
    "        # time.sleep(0.001 * self.m / self.n if self.n > 0 else 0) # Avoid realistic simulation\n",
    "        return [] # Cannot return a valid forest\n",
    "\n",
    "    def _placeholder_decode_bipartiteness(self):\n",
    "        \"\"\"Placeholder: Real decoding needed.\"\"\"\n",
    "        print(\"[WARN] Using placeholder decode_bipartiteness. Returning default.\")\n",
    "        return True # Cannot determine bipartiteness\n",
    "\n",
    "    def extract_spanning_forest(self):\n",
    "        \"\"\"Uses placeholder decoder.\"\"\"\n",
    "        print(f\"[INFO] Calling placeholder decode_sketch for forest extraction...\")\n",
    "        candidate_edges = self._placeholder_decode_sketch() # Gets []\n",
    "\n",
    "        # The rest of the function attempts to build a forest from candidates.\n",
    "        # Since candidates is empty, it will return an empty forest.\n",
    "        if self.n == 0: return []\n",
    "        parent = list(range(self.n))\n",
    "        # Simplified find without path compression for clarity in placeholder context\n",
    "        def find(i):\n",
    "            while parent[i] != i:\n",
    "                i = parent[i]\n",
    "            return i\n",
    "        def union(i, j, edge_data): # Added edge_data to potentially use weight 'w'\n",
    "            root_i = find(i)\n",
    "            root_j = find(j)\n",
    "            if root_i != root_j:\n",
    "                parent[root_j] = root_i\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        forest = []\n",
    "        num_edges_in_forest = 0\n",
    "        # Assuming candidate_edges format is (u, v, w) or (u, v)\n",
    "        # Sort candidates if weights matter (e.g., for MST approximation)\n",
    "        # sorted_candidates = sorted(candidate_edges, key=lambda x: x[2] if len(x) > 2 else 1)\n",
    "\n",
    "        for edge_data in candidate_edges: # This loop will not run if placeholder returns []\n",
    "            if len(edge_data) < 2: continue # Skip malformed entries\n",
    "            u, v = edge_data[0], edge_data[1]\n",
    "            # Basic validation\n",
    "            if not (0 <= u < self.n and 0 <= v < self.n):\n",
    "                 print(f\"[WARN] Skipping invalid edge from decoder: ({u}, {v})\")\n",
    "                 continue\n",
    "\n",
    "            if union(u, v, edge_data):\n",
    "                forest.append(edge_data) # Append the original tuple/list\n",
    "                num_edges_in_forest += 1\n",
    "                # If building MST and candidates were sorted, could stop early\n",
    "                # if num_edges_in_forest == self.n - 1: break # Only if n > 0\n",
    "\n",
    "        # Returns empty list due to placeholder decoder\n",
    "        return forest\n",
    "\n",
    "    def is_connected(self):\n",
    "        \"\"\"Checks connectivity using placeholder decoder. Will likely return False.\"\"\"\n",
    "        if self.n <= 0: return False # Consistent with n=0 case\n",
    "        if self.n == 1: return True\n",
    "        # Relies on extract_spanning_forest, which uses the placeholder.\n",
    "        # The placeholder returns [], so len(forest) will be 0.\n",
    "        # Thus, this will return False unless n=1.\n",
    "        forest = self.extract_spanning_forest()\n",
    "        # Check number of edges found by the (placeholder) decoder\n",
    "        num_edges_in_forest = len(forest)\n",
    "        # The theoretical check is if the *true* MST has n-1 edges.\n",
    "        # Our check is based on what the (placeholder) decoder returned.\n",
    "        # This is NOT a reliable connectivity test with the placeholder.\n",
    "        print(\"[WARN] is_connected result relies on placeholder decoder.\")\n",
    "        return num_edges_in_forest == self.n - 1\n",
    "\n",
    "\n",
    "    def is_bipartite(self):\n",
    "        \"\"\"Checks bipartiteness using placeholder decoder.\"\"\"\n",
    "        print(f\"[INFO] Calling placeholder decode_bipartiteness...\")\n",
    "        print(\"[WARN] is_bipartite result relies on placeholder decoder.\")\n",
    "        return self._placeholder_decode_bipartiteness() # Returns True currently\n",
    "\n",
    "    def approx_mst_weight(self):\n",
    "        \"\"\"Approximates MST weight using placeholder decoder. Returns 0.\"\"\"\n",
    "        # This relies on extract_spanning_forest which uses the placeholder.\n",
    "        # Placeholder returns [], so the sum will be 0.\n",
    "        forest = self.extract_spanning_forest()\n",
    "        # Assumes forest edges are tuples like (u, v, w) where w is weight\n",
    "        # If weights are not present, assumes weight 1 or requires modification\n",
    "        total_weight = sum(edge[2] for edge in forest if len(edge) > 2) # Sum weights if available\n",
    "        # If format is just (u,v), the sum is 0. If (u,v,w) is returned by decoder, it sums w.\n",
    "        # Since placeholder returns [], sum is 0.\n",
    "        print(\"[WARN] approx_mst_weight result relies on placeholder decoder.\")\n",
    "        return total_weight\n",
    "\n",
    "    def get_memory_estimate(self):\n",
    "        # Estimate memory for the sketch vector y and the sign dictionaries\n",
    "        y_mem = sys.getsizeof(self.y) + sum(sys.getsizeof(x) for x in self.y)\n",
    "        signs_mem = sys.getsizeof(self.signs)\n",
    "        for sign_dict in self.signs:\n",
    "            signs_mem += sys.getsizeof(sign_dict)\n",
    "            # Estimate memory of keys (edges) and values (signs) within each dict\n",
    "            # This can be expensive if dicts are large, do a sample or approximation\n",
    "            # For simplicity, just add size of dict object itself\n",
    "            # A more detailed sum:\n",
    "            # for k, v in sign_dict.items():\n",
    "            #    signs_mem += sys.getsizeof(k) + sys.getsizeof(v)\n",
    "        return y_mem + signs_mem\n",
    "\n",
    "# --- DSU Class (for Boruvka, Kruskal) ---\n",
    "class DSU:\n",
    "    # ... (DSU class implementation from original code) ...\n",
    "    def __init__(self, n):\n",
    "        if n < 0: raise ValueError(\"Number of elements n cannot be negative.\")\n",
    "        self.parent = list(range(n))\n",
    "        self.size = [1] * n # Used for union by size/rank heuristic\n",
    "        self.num_sets = n # Number of disjoint sets\n",
    "\n",
    "    def find(self, i):\n",
    "        # Find with path compression\n",
    "        if i < 0 or i >= len(self.parent):\n",
    "             raise IndexError(\"DSU index out of bounds.\")\n",
    "        if self.parent[i] == i:\n",
    "            return i\n",
    "        self.parent[i] = self.find(self.parent[i]) # Path compression\n",
    "        return self.parent[i]\n",
    "\n",
    "    def union(self, i, j):\n",
    "        # Union by size\n",
    "        if i < 0 or i >= len(self.parent) or j < 0 or j >= len(self.parent):\n",
    "             raise IndexError(\"DSU index out of bounds.\")\n",
    "        root_i = self.find(i)\n",
    "        root_j = self.find(j)\n",
    "        if root_i != root_j:\n",
    "            # Merge smaller tree into larger tree\n",
    "            if self.size[root_i] < self.size[root_j]:\n",
    "                root_i, root_j = root_j, root_i # Swap roots\n",
    "            self.parent[root_j] = root_i\n",
    "            self.size[root_i] += self.size[root_j]\n",
    "            self.num_sets -= 1\n",
    "            return True # Return True if merge happened\n",
    "        return False # Return False if already in the same set\n",
    "\n",
    "    def get_num_sets(self):\n",
    "        return self.num_sets\n",
    "\n",
    "# --- ModifiedBoruvkaSketching Class ---\n",
    "class ModifiedBoruvkaSketching:\n",
    "    \"\"\"\n",
    "    Conceptual: Leverages sketches to find minimum weight edge out of components.\n",
    "    Actual Implementation: Uses a fallback simple Boruvka on the full edge list\n",
    "                           stored in a heap, as sketch integration is not implemented.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_nodes):\n",
    "        if num_nodes < 0: raise ValueError(\"Number of nodes cannot be negative.\")\n",
    "        self.num_nodes = num_nodes\n",
    "        # Store edges in a list or heap if processing offline after adding all edges\n",
    "        self.edges = [] # Using a list, will convert to heap or sort later if needed\n",
    "        print(f\"Initialized ModifiedBoruvkaSketching for {self.num_nodes} nodes.\")\n",
    "        # Placeholder for conceptual sketches (not used in current find_spanning_tree)\n",
    "        # self.sketches_per_node = [...]\n",
    "\n",
    "    def add_edge(self, u, v, weight):\n",
    "        \"\"\"Adds edge (for later processing).\"\"\"\n",
    "        if not (0 <= u < self.num_nodes and 0 <= v < self.num_nodes):\n",
    "             print(f\"Warning: Edge ({u}, {v}) ignored, nodes out of range [0, {self.num_nodes-1}]\")\n",
    "             return\n",
    "        if u == v: return # Ignore self-loops for standard MST\n",
    "        # Store edge with weight first for easy sorting/heap processing\n",
    "        heapq.heappush(self.edges, (weight, u, v))\n",
    "\n",
    "    def delete_edge(self, u, v, weight):\n",
    "        \"\"\"Conceptual deletion. Not supported by the current fallback implementation.\"\"\"\n",
    "        print(f\"[WARN] delete_edge not supported in fallback Boruvka. Edge ({u},{v},{weight}) ignored.\")\n",
    "        # To support deletion, would need to rebuild the edge list/heap or use complex structures.\n",
    "\n",
    "\n",
    "    def find_spanning_tree(self):\n",
    "        \"\"\"\n",
    "        Computes MST using the fallback Boruvka's algorithm on the stored edges.\n",
    "        Does NOT use the conceptual 'sketches' mentioned in comments/theory.\n",
    "        \"\"\"\n",
    "        if self.num_nodes == 0: return [], 0\n",
    "        dsu = DSU(self.num_nodes)\n",
    "        mst_edges = []\n",
    "        mst_weight = 0\n",
    "        # Make a copy or work with the heap directly if edge stream was large\n",
    "        # For simplicity, assume self.edges contains all edges now.\n",
    "        # Convert the list `self.edges` into a structure suitable for Boruvka rounds.\n",
    "        # A simple list that we iterate through each round works, but is inefficient.\n",
    "        # A copy of the edges allows filtering as the algorithm progresses.\n",
    "        edge_list_for_rounds = sorted(self.edges) # Sort once if iterating multiple times\n",
    "\n",
    "        print(f\"\\nStarting Fallback Boruvka's Algorithm on {len(edge_list_for_rounds)} edges...\")\n",
    "        num_components = self.num_nodes\n",
    "\n",
    "        while num_components > 1:\n",
    "            # cheapest_edge_map[component_root] = (min_weight, u, v)\n",
    "            cheapest_edge_map = {}\n",
    "\n",
    "            # Find the cheapest edge leaving each component\n",
    "            # Iterate through *all* remaining edges (inefficient part of fallback)\n",
    "            edges_processed_this_round = 0\n",
    "            for weight, u, v in edge_list_for_rounds: # Inefficiently iterates all edges each round\n",
    "                root_u = dsu.find(u)\n",
    "                root_v = dsu.find(v)\n",
    "                edges_processed_this_round += 1\n",
    "\n",
    "                if root_u != root_v:\n",
    "                    # Edge connects two different components\n",
    "                    # Update cheapest edge for component root_u if this edge is cheaper\n",
    "                    if root_u not in cheapest_edge_map or weight < cheapest_edge_map[root_u][0]:\n",
    "                        cheapest_edge_map[root_u] = (weight, u, v)\n",
    "                    # Update cheapest edge for component root_v if this edge is cheaper\n",
    "                    if root_v not in cheapest_edge_map or weight < cheapest_edge_map[root_v][0]:\n",
    "                        cheapest_edge_map[root_v] = (weight, u, v)\n",
    "\n",
    "            print(f\"Round: {self.num_nodes - num_components + 1}, Components: {num_components}, Edges considered: {edges_processed_this_round}, Cheapest edges found: {len(cheapest_edge_map)}\")\n",
    "\n",
    "            if not cheapest_edge_map:\n",
    "                 # No edges found connecting components, graph must be disconnected\n",
    "                 print(\"Warning: No connecting edges found. Graph may be disconnected.\")\n",
    "                 break # Exit loop\n",
    "\n",
    "            num_merges_this_round = 0\n",
    "            # Process the cheapest edges found for each component to merge them\n",
    "            for root_node, edge_data in cheapest_edge_map.items():\n",
    "                weight, u, v = edge_data\n",
    "                # Perform union operation. DSU handles check if already merged.\n",
    "                if dsu.union(u, v):\n",
    "                    # print(f\"  Merging components of {u} and {v} using edge ({u}, {v}, {weight})\")\n",
    "                    mst_edges.append((u, v, weight))\n",
    "                    mst_weight += weight\n",
    "                    num_merges_this_round += 1\n",
    "                    num_components -= 1 # Decrement component count upon successful merge\n",
    "\n",
    "\n",
    "            if num_merges_this_round == 0:\n",
    "                # This happens if all cheapest edges connect components already merged\n",
    "                # in *this same round* by another edge. Graph might be disconnected if num_components > 1.\n",
    "                 if num_components > 1:\n",
    "                      print(\"Warning: No merges occurred in this round, but multiple components remain.\")\n",
    "                 break # Avoid infinite loop\n",
    "\n",
    "        print(\"Boruvka's algorithm finished.\")\n",
    "        if num_components == 1:\n",
    "            print(f\"Spanning tree found with total weight: {mst_weight:.2f}\")\n",
    "        else:\n",
    "            print(f\"Could not find a spanning tree. {num_components} components remain.\")\n",
    "            # Return the forest found so far\n",
    "        return mst_edges, mst_weight\n",
    "\n",
    "\n",
    "    def get_memory_estimate(self):\n",
    "        # Memory is dominated by the stored edges heap/list\n",
    "        base_mem = sys.getsizeof(self.num_nodes) + sys.getsizeof(self.edges)\n",
    "        edges_mem = sum(sys.getsizeof(e) + sys.getsizeof(e[0]) + sys.getsizeof(e[1]) + sys.getsizeof(e[2]) for e in self.edges)\n",
    "        return base_mem + edges_mem\n",
    "\n",
    "\n",
    "# --- AdaptiveGraphSketch Class ---\n",
    "class AdaptiveGraphSketch:\n",
    "    \"\"\"\n",
    "    Implements adaptive sparsification via random sampling over multiple rounds.\n",
    "    Computes exact MST and approximate Max-Weight Matching on the final sparsifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, n, gamma=0.1):\n",
    "        if n < 0: raise ValueError(\"Number of vertices n cannot be negative.\")\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError(\"gamma must be between 0 (exclusive) and 1 (inclusive)\")\n",
    "        self.n = n\n",
    "        self.gamma = gamma\n",
    "        # Number of rounds, ensure at least 1\n",
    "        self.rounds = max(1, int(math.ceil(1.0 / gamma)))\n",
    "\n",
    "        # Store graph as adjacency list: {u: {v: weight, ...}}\n",
    "        self.current_graph = defaultdict(dict)\n",
    "        self.edge_count = 0 # Number of unique edges in current_graph\n",
    "\n",
    "    def stream_update(self, u, v, w):\n",
    "        \"\"\"Add/update edge (u, v) with weight w.\"\"\"\n",
    "        if not (0 <= u < self.n and 0 <= v < self.n):\n",
    "            # Silently ignore or raise error for out-of-bounds nodes\n",
    "            # print(f\"Warning: Skipping update for edge ({u},{v}), nodes out of range [0, {self.n-1}]\")\n",
    "            return\n",
    "        if u == v: return # Ignore self-loops\n",
    "\n",
    "        # Check if edge is new before adding to avoid double counting\n",
    "        is_new_edge = v not in self.current_graph[u]\n",
    "\n",
    "        # Update/add edge in both directions for undirected graph\n",
    "        self.current_graph[u][v] = w\n",
    "        self.current_graph[v][u] = w\n",
    "\n",
    "        if is_new_edge:\n",
    "            self.edge_count += 1\n",
    "\n",
    "    def build_sparsifier(self):\n",
    "        \"\"\"Performs one round of random edge sampling sparsification.\"\"\"\n",
    "        if self.n <= 1 or self.edge_count == 0:\n",
    "            # No sparsification needed for trivial graphs or if no edges\n",
    "            return\n",
    "\n",
    "        # Sampling probability p = n^{-gamma} (or 1 if n=1)\n",
    "        # Ensure p is not zero and handle potential floating point issues\n",
    "        p = self.n ** (-self.gamma) if self.n > 1 else 1.0\n",
    "        # Clamp p to avoid issues if gamma is very large or n is huge\n",
    "        p = max(min(p, 1.0), 1e-9) # Avoid p=0 division error, ensure p<=1\n",
    "\n",
    "        new_graph = defaultdict(dict)\n",
    "        new_edge_count = 0\n",
    "        processed_edges = set() # Track processed edges to handle undirected nature\n",
    "\n",
    "        for u, neighbors in self.current_graph.items():\n",
    "            for v, w in neighbors.items():\n",
    "                # Ensure we process each undirected edge only once\n",
    "                edge = tuple(sorted((u, v)))\n",
    "                if edge in processed_edges:\n",
    "                    continue\n",
    "                processed_edges.add(edge)\n",
    "\n",
    "                # Sample the edge\n",
    "                if random.random() < p:\n",
    "                    # Keep the edge and rescale its weight\n",
    "                    rescaled_w = w / p\n",
    "                    new_graph[u][v] = rescaled_w\n",
    "                    new_graph[v][u] = rescaled_w\n",
    "                    new_edge_count += 1\n",
    "\n",
    "        # Update graph and edge count\n",
    "        self.current_graph = new_graph\n",
    "        self.edge_count = new_edge_count\n",
    "\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"Runs sparsification rounds and computes final results.\"\"\"\n",
    "        print(f\"[INFO] AdaptiveSketch: Starting {self.rounds} rounds of sparsification (gamma={self.gamma})...\")\n",
    "        initial_edge_count = self.edge_count\n",
    "        for r in range(self.rounds):\n",
    "            round_start_edges = self.edge_count\n",
    "            self.build_sparsifier()\n",
    "            print(f\"  Round {r+1}/{self.rounds}: Sparsified from {round_start_edges} to {self.edge_count} edges.\")\n",
    "            if self.edge_count == 0: break # Stop if no edges left\n",
    "\n",
    "        print(f\"[INFO] Final sparsifier has {self.edge_count} edges (started with {initial_edge_count}).\")\n",
    "\n",
    "        print(\"[INFO] Computing exact MST on the sparsifier...\")\n",
    "        start_time = time.time()\n",
    "        mst_weight = self._exact_mst()\n",
    "        mst_time = time.time() - start_time\n",
    "        print(f\"[INFO]   MST Weight on sparsifier: {mst_weight:.2f} (computed in {mst_time:.4f}s)\")\n",
    "\n",
    "        print(\"[INFO] Computing approximate max-weight matching on the sparsifier...\")\n",
    "        start_time = time.time()\n",
    "        matching = self._approx_max_matching()\n",
    "        matching_time = time.time() - start_time\n",
    "        matching_weight = sum(w for _, _, w in matching)\n",
    "        print(f\"[INFO]   Approx Max-Weight Matching size: {len(matching)}, Total Weight: {matching_weight:.2f} (computed in {matching_time:.4f}s)\")\n",
    "\n",
    "        # Return results and timing info perhaps\n",
    "        return mst_weight, matching, mst_time, matching_time\n",
    "\n",
    "    def _get_edges_from_sparsifier(self):\n",
    "        \"\"\"Helper to extract unique edges (weight, u, v) for sorting.\"\"\"\n",
    "        edges = []\n",
    "        processed = set()\n",
    "        for u, neighbors in self.current_graph.items():\n",
    "            for v, w in neighbors.items():\n",
    "                edge = tuple(sorted((u, v)))\n",
    "                if edge not in processed:\n",
    "                    # Store weight first for easy sorting by weight\n",
    "                    edges.append((w, u, v))\n",
    "                    processed.add(edge)\n",
    "        return edges\n",
    "\n",
    "    def _exact_mst(self):\n",
    "        \"\"\"Computes exact MST weight on the current sparsifier using Kruskal.\"\"\"\n",
    "        if self.n == 0 or self.edge_count == 0: return 0.0\n",
    "\n",
    "        edges = self._get_edges_from_sparsifier()\n",
    "        edges.sort() # Sort by weight (ascending)\n",
    "\n",
    "        dsu = DSU(self.n)\n",
    "        mst_total_weight = 0.0\n",
    "        mst_edge_count = 0\n",
    "        max_edges_in_mst = self.n - 1 if self.n > 0 else 0\n",
    "\n",
    "        for w, u, v in edges:\n",
    "            if dsu.union(u, v):\n",
    "                mst_total_weight += w\n",
    "                mst_edge_count += 1\n",
    "                if mst_edge_count == max_edges_in_mst:\n",
    "                    # Found a spanning tree (if graph was connected)\n",
    "                    break # Optimization\n",
    "\n",
    "        # Check if a spanning tree was actually formed\n",
    "        # if mst_edge_count != max_edges_in_mst and self.n > 1:\n",
    "        #     # This can happen if the sparsifier became disconnected\n",
    "        #     print(f\"[WARN] Sparsifier may be disconnected; MST calculation formed a forest with {mst_edge_count} edges.\")\n",
    "\n",
    "        return mst_total_weight\n",
    "\n",
    "    def _approx_max_matching(self):\n",
    "        \"\"\"Computes 1/2-approx max weight matching using greedy approach.\"\"\"\n",
    "        if self.n == 0 or self.edge_count == 0: return []\n",
    "\n",
    "        edges = self._get_edges_from_sparsifier()\n",
    "        # Sort by weight descending for greedy selection\n",
    "        edges.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        matched = [False] * self.n # Track matched vertices\n",
    "        matching_result = []\n",
    "\n",
    "        for w, u, v in edges:\n",
    "            # If both endpoints are available, add edge to matching\n",
    "            if not matched[u] and not matched[v]:\n",
    "                matched[u] = True\n",
    "                matched[v] = True\n",
    "                matching_result.append((u, v, w)) # Store edge info\n",
    "\n",
    "        return matching_result\n",
    "\n",
    "\n",
    "    def get_memory_estimate(self):\n",
    "        # Memory is dominated by the current_graph structure\n",
    "        graph_mem = sys.getsizeof(self.current_graph)\n",
    "        edge_mem = 0\n",
    "        for u, neighbors in self.current_graph.items():\n",
    "            graph_mem += sys.getsizeof(u) + sys.getsizeof(neighbors)\n",
    "            for v, w in neighbors.items():\n",
    "                graph_mem += sys.getsizeof(v) + sys.getsizeof(w)\n",
    "                # Edge data is stored twice (u->v and v->u), count size once roughly\n",
    "                edge_mem += sys.getsizeof(v) + sys.getsizeof(w) # Approximate\n",
    "\n",
    "        # Other small variables\n",
    "        other_mem = sys.getsizeof(self.n) + sys.getsizeof(self.gamma) + \\\n",
    "                    sys.getsizeof(self.rounds) + sys.getsizeof(self.edge_count)\n",
    "\n",
    "        # Return sum, graph_mem includes overhead of dicts/sets\n",
    "        # A simple estimate might be proportional to n + edge_count\n",
    "        estimated_size = sys.getsizeof(self.current_graph) + self.edge_count * (sys.getsizeof(0) * 2 + sys.getsizeof(0.0)) # Rough estimate\n",
    "        return estimated_size\n",
    "\n",
    "\n",
    "# --- SimpleSetSketch Class ---\n",
    "class SimpleSetSketch:\n",
    "    def __init__(self, sketch_size, num_hashes=3):\n",
    "        if sketch_size <= 0: raise ValueError(\"Sketch size must be positive\")\n",
    "        if num_hashes <= 0: raise ValueError(\"Number of hashes must be positive\")\n",
    "\n",
    "        self.size = sketch_size\n",
    "        self.num_hashes = num_hashes\n",
    "        self.sketch_array = [0] * self.size\n",
    "        # Use fixed seeds for reproducibility if needed, or random per instance\n",
    "        self._hash_seeds = [random.randint(0, 2**32 - 1) for _ in range(num_hashes)]\n",
    "\n",
    "    def _get_hash_indices(self, element):\n",
    "        \"\"\"Calculates hash indices and integer representation for XORing.\"\"\"\n",
    "        indices = set()\n",
    "        # Convert element to bytes consistently\n",
    "        try:\n",
    "            if isinstance(element, bytes):\n",
    "                element_bytes = element\n",
    "            elif isinstance(element, str):\n",
    "                element_bytes = element.encode('utf-8')\n",
    "            elif isinstance(element, int):\n",
    "                 element_bytes = element.to_bytes((element.bit_length() + 7) // 8, 'big')\n",
    "                 if element == 0: element_bytes = b'\\x00' # Handle zero case\n",
    "            elif isinstance(element, tuple):\n",
    "                 # Hash tuple representation; assumes elements are hashable\n",
    "                 element_bytes = str(element).encode('utf-8') # Simple string conversion\n",
    "            else:\n",
    "                # Fallback: use string representation\n",
    "                element_bytes = str(element).encode('utf-8')\n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Could not convert element {element} to bytes: {e}. Using hash().\")\n",
    "             # Fallback if direct byte conversion fails\n",
    "             element_bytes = str(hash(element)).encode('utf-8')\n",
    "\n",
    "\n",
    "        # Use hash() for XOR value, try to make it consistent\n",
    "        try:\n",
    "            element_int_repr = hash(element)\n",
    "        except TypeError:\n",
    "            # If element is not hashable (e.g., list), use its string representation's hash\n",
    "            element_int_repr = hash(str(element))\n",
    "\n",
    "        # Generate hash indices using the seeds\n",
    "        for i in range(self.num_hashes):\n",
    "            # Combine element bytes and seed, then hash\n",
    "            hasher = hashlib.sha256() # Use a standard hash like SHA-256\n",
    "            hasher.update(element_bytes)\n",
    "            hasher.update(self._hash_seeds[i].to_bytes(4, 'big')) # Mix in seed\n",
    "            hash_val = int(hasher.hexdigest(), 16)\n",
    "            indices.add(hash_val % self.size) # Map to sketch index\n",
    "\n",
    "        return list(indices), element_int_repr\n",
    "\n",
    "    def add(self, element):\n",
    "        \"\"\"Adds an element to the sketch using XOR.\"\"\"\n",
    "        indices, element_int_repr = self._get_hash_indices(element)\n",
    "        for i in indices:\n",
    "            if 0 <= i < self.size: # Bounds check\n",
    "                 self.sketch_array[i] ^= element_int_repr\n",
    "\n",
    "    def merge(self, other_sketch):\n",
    "        \"\"\"Merges another sketch into this one using XOR.\"\"\"\n",
    "        if self.size != other_sketch.size or self.num_hashes != other_sketch.num_hashes:\n",
    "            raise ValueError(\"Sketches must have the same size and number of hashes.\")\n",
    "        # Also might want to check if hash seeds are the same if strict merging is required\n",
    "        # if self._hash_seeds != other_sketch._hash_seeds:\n",
    "        #     print(\"Warning: Merging sketches with different hash seeds.\")\n",
    "\n",
    "        for i in range(self.size):\n",
    "            self.sketch_array[i] ^= other_sketch.sketch_array[i]\n",
    "\n",
    "    def get_sketch(self):\n",
    "        \"\"\"Returns the current sketch array.\"\"\"\n",
    "        return list(self.sketch_array) # Return a copy\n",
    "\n",
    "    def get_memory_estimate(self):\n",
    "        # Memory is dominated by the sketch array\n",
    "        array_mem = sys.getsizeof(self.sketch_array) + sum(sys.getsizeof(x) for x in self.sketch_array)\n",
    "        seeds_mem = sys.getsizeof(self._hash_seeds) + sum(sys.getsizeof(x) for x in self._hash_seeds)\n",
    "        other_mem = sys.getsizeof(self.size) + sys.getsizeof(self.num_hashes)\n",
    "        return array_mem + seeds_mem + other_mem\n",
    "\n",
    "# --- CountMinSketch Class ---\n",
    "class CountMinSketch:\n",
    "    def __init__(self, width, depth, seed=None):\n",
    "        if not (width > 0 and depth > 0):\n",
    "            raise ValueError(\"Width and depth must be positive integers\")\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.table = [[0] * width for _ in range(depth)]\n",
    "\n",
    "        # Initialize hash seeds based on the provided seed for reproducibility\n",
    "        if seed is not None:\n",
    "             random.seed(seed)\n",
    "        self._hash_seeds = [random.randint(0, 2**32 - 1) for _ in range(depth)]\n",
    "\n",
    "    def _get_hashes(self, item):\n",
    "        \"\"\"Get hash indices for the item for each row (depth).\"\"\"\n",
    "        hashes = []\n",
    "        # Convert item to bytes consistently for hashing\n",
    "        try:\n",
    "            item_str = str(item)\n",
    "            item_bytes = item_str.encode('utf-8')\n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Could not encode item {item} to bytes: {e}. Using fallback.\")\n",
    "             item_bytes = b\"fallback_item_representation\"\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            hasher = hashlib.sha256()\n",
    "            hasher.update(item_bytes)\n",
    "            hasher.update(self._hash_seeds[i].to_bytes(4, 'big')) # Mix in seed\n",
    "            hash_val = int(hasher.hexdigest(), 16)\n",
    "            hashes.append(hash_val % self.width) # Map to table width\n",
    "        return hashes\n",
    "\n",
    "    def add(self, item, count=1):\n",
    "        \"\"\"Increment the count for an item.\"\"\"\n",
    "        if count == 0: return\n",
    "        hashes = self._get_hashes(item)\n",
    "        for i in range(self.depth):\n",
    "             if 0 <= hashes[i] < self.width: # Bounds check\n",
    "                 self.table[i][hashes[i]] += count\n",
    "             else: # Should not happen with modulo arithmetic, but safety first\n",
    "                 print(f\"Warning: Hash index {hashes[i]} out of bounds for width {self.width}.\")\n",
    "\n",
    "    def estimate(self, item):\n",
    "        \"\"\"Estimate the count (frequency) of an item.\"\"\"\n",
    "        hashes = self._get_hashes(item)\n",
    "        min_count = float('inf')\n",
    "        for i in range(self.depth):\n",
    "             if 0 <= hashes[i] < self.width: # Bounds check\n",
    "                 min_count = min(min_count, self.table[i][hashes[i]])\n",
    "             else: # Should not happen\n",
    "                 print(f\"Warning: Hash index {hashes[i]} out of bounds during estimation.\")\n",
    "                 return 0 # Return 0 or raise error if index is invalid\n",
    "        # Handle case where item was never added (min_count remains inf)\n",
    "        return min_count if min_count != float('inf') else 0\n",
    "\n",
    "    def get_memory_estimate(self):\n",
    "        # Memory is dominated by the table\n",
    "        table_mem = sys.getsizeof(self.table)\n",
    "        for row in self.table:\n",
    "            table_mem += sys.getsizeof(row) + sum(sys.getsizeof(x) for x in row)\n",
    "        seeds_mem = sys.getsizeof(self._hash_seeds) + sum(sys.getsizeof(x) for x in self._hash_seeds)\n",
    "        other_mem = sys.getsizeof(self.width) + sys.getsizeof(self.depth)\n",
    "        return table_mem + seeds_mem + other_mem\n",
    "\n",
    "# --- gSketch Class ---\n",
    "class gSketch:\n",
    "    \"\"\"\n",
    "    Simulates gSketch using partitioned CountMinSketches for localized queries\n",
    "    (e.g., node degree estimation).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_partitions, cms_width, cms_depth, node_ids=None, query_workload=None):\n",
    "        if num_partitions <= 0: raise ValueError(\"Number of partitions must be positive.\")\n",
    "        if cms_width <= 0 or cms_depth <= 0: raise ValueError(\"CMS width and depth must be positive.\")\n",
    "\n",
    "        self.num_partitions = num_partitions\n",
    "        # Create separate CMS for each partition with unique seeds\n",
    "        self.partitions = [CountMinSketch(cms_width, cms_depth, seed=i) for i in range(num_partitions)]\n",
    "        # Cache node to partition mapping for efficiency\n",
    "        self.node_to_partition = {}\n",
    "\n",
    "        print(f\"Initialized gSketch with {num_partitions} partitions.\")\n",
    "        print(f\"Each partition uses a CountMinSketch(w={cms_width}, d={cms_depth}).\")\n",
    "\n",
    "        # Conceptual handling of query workload (not implemented)\n",
    "        if query_workload:\n",
    "            print(\"Conceptual: Query workload provided (details not used in this impl).\")\n",
    "            # Logic would go here to potentially adjust partitioning or CMS parameters\n",
    "            pass # Placeholder\n",
    "\n",
    "        # Pre-assign partitions if node IDs are known upfront\n",
    "        if node_ids:\n",
    "            print(f\"Pre-assigning {len(node_ids)} nodes to partitions...\")\n",
    "            for node_id in node_ids:\n",
    "                self._assign_partition(node_id) # Assigns and caches\n",
    "\n",
    "    def _get_partition_index(self, node_id):\n",
    "        \"\"\"Determines partition index using a hash function.\"\"\"\n",
    "        # Use a stable hash based on the node ID\n",
    "        try:\n",
    "            node_str = str(node_id)\n",
    "            node_bytes = node_str.encode('utf-8')\n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Could not encode node ID {node_id} to bytes: {e}. Using fallback.\")\n",
    "             node_bytes = b\"fallback_node_id\"\n",
    "\n",
    "        hasher = hashlib.sha256()\n",
    "        hasher.update(node_bytes)\n",
    "        # Add a fixed salt/seed specific to gSketch partitioning if needed\n",
    "        # hasher.update(b\"gSketchPartitionSalt\")\n",
    "        hash_val = int(hasher.hexdigest(), 16)\n",
    "        return hash_val % self.num_partitions\n",
    "\n",
    "    def _assign_partition(self, node_id):\n",
    "        \"\"\"Assigns node to a partition if not already assigned. Returns partition index.\"\"\"\n",
    "        if node_id not in self.node_to_partition:\n",
    "            part_idx = self._get_partition_index(node_id)\n",
    "            self.node_to_partition[node_id] = part_idx\n",
    "        return self.node_to_partition[node_id]\n",
    "\n",
    "    def add_edge(self, u, v, weight=1):\n",
    "        \"\"\"\n",
    "        Processes edge update for degree estimation.\n",
    "        Increments degree count for both u and v in their respective partition sketches.\n",
    "        \"\"\"\n",
    "        if weight == 0: return # No change\n",
    "\n",
    "        # Assign nodes to partitions (uses cache if already assigned)\n",
    "        part_idx_u = self._assign_partition(u)\n",
    "        part_idx_v = self._assign_partition(v)\n",
    "\n",
    "        # Update the CMS in the assigned partition for each node.\n",
    "        # The item added to CMS is the node ID itself, representing its degree count.\n",
    "        if 0 <= part_idx_u < self.num_partitions:\n",
    "            self.partitions[part_idx_u].add(u, weight) # Increment degree count for u\n",
    "        else: # Should not happen\n",
    "             print(f\"Warning: Invalid partition index {part_idx_u} for node {u}.\")\n",
    "\n",
    "        # Avoid double counting if u and v are the same node (self-loop)\n",
    "        # and fall into the same partition (though self-loops often ignored)\n",
    "        if u == v: return # Or handle self-loops based on requirements\n",
    "\n",
    "        if 0 <= part_idx_v < self.num_partitions:\n",
    "            self.partitions[part_idx_v].add(v, weight) # Increment degree count for v\n",
    "        else: # Should not happen\n",
    "             print(f\"Warning: Invalid partition index {part_idx_v} for node {v}.\")\n",
    "\n",
    "\n",
    "    def estimate_degree(self, node_id):\n",
    "        \"\"\"Estimates degree of node_id using its assigned partition's CMS.\"\"\"\n",
    "        if node_id not in self.node_to_partition:\n",
    "            # Node was never added via an edge, or not pre-assigned.\n",
    "            # Assign it now to find its potential partition, but estimate will be 0.\n",
    "            print(f\"Info: Node {node_id} not seen before. Assigning partition. Degree estimated as 0.\")\n",
    "            self._assign_partition(node_id) # Assign for consistency, though degree is 0\n",
    "            return 0\n",
    "\n",
    "        part_idx = self.node_to_partition[node_id]\n",
    "        if 0 <= part_idx < self.num_partitions:\n",
    "            # Estimate the count of 'node_id' in its partition's CMS\n",
    "            estimated_degree = self.partitions[part_idx].estimate(node_id)\n",
    "            return estimated_degree\n",
    "        else: # Should not happen\n",
    "             print(f\"Warning: Node {node_id} has invalid partition index {part_idx}. Returning 0.\")\n",
    "             return 0\n",
    "\n",
    "    def get_memory_estimate(self):\n",
    "        base_mem = sys.getsizeof(self.num_partitions) + sys.getsizeof(self.partitions) + \\\n",
    "                   sys.getsizeof(self.node_to_partition)\n",
    "        partitions_mem = sum(p.get_memory_estimate() for p in self.partitions)\n",
    "        # Estimate node_to_partition dict memory (crude)\n",
    "        map_mem = sum(sys.getsizeof(k) + sys.getsizeof(v) for k, v in self.node_to_partition.items())\n",
    "        return base_mem + partitions_mem + map_mem\n",
    "\n",
    "\n",
    "# --- GraphVisualSketchHierarchy Class ---\n",
    "class GraphVisualSketchHierarchy:\n",
    "    \"\"\"Builds a multi-level graph hierarchy by iterative node aggregation.\"\"\"\n",
    "    def __init__(self, original_graph_adj):\n",
    "        \"\"\"Initializes with the original graph (Level 0).\"\"\"\n",
    "        if not isinstance(original_graph_adj, dict):\n",
    "            raise TypeError(\"original_graph_adj must be a dictionary (adjacency list).\")\n",
    "\n",
    "        # Ensure values are sets for consistent neighbor representation\n",
    "        self.levels = {0: self._deep_copy_adj_to_sets(original_graph_adj)}\n",
    "        self.max_level = 0\n",
    "        # node_mapping[level][supernode_id] = {original_node_id1, ...}\n",
    "        self.node_mapping = {0: {node: {node} for node in original_graph_adj}}\n",
    "\n",
    "        print(\"Initialized Visual Sketch Hierarchy with Level 0.\")\n",
    "\n",
    "    def _deep_copy_adj_to_sets(self, adj):\n",
    "        # Creates a deep copy ensuring neighbors are stored in sets\n",
    "        copied_adj = {}\n",
    "        for node, neighbors in adj.items():\n",
    "            try:\n",
    "                 copied_adj[node] = set(neighbors)\n",
    "            except TypeError:\n",
    "                 print(f\"Warning: Neighbors of node {node} is not iterable. Setting to empty set.\")\n",
    "                 copied_adj[node] = set()\n",
    "        return copied_adj\n",
    "\n",
    "    def _create_next_level(self, current_level_adj, max_nodes_per_supernode=None):\n",
    "        \"\"\"\n",
    "        Creates the next hierarchy level by merging connected nodes/supernodes.\n",
    "        Simplified Strategy: Iteratively merge a node with one of its neighbors\n",
    "                             if constraints (like max_nodes_per_supernode) are met.\n",
    "                             This is a basic approach; more sophisticated methods exist\n",
    "                             (e.g., using community detection, label propagation).\n",
    "        \"\"\"\n",
    "        current_level = self.max_level\n",
    "        print(f\"\\nAttempting to create Level {current_level + 1}...\")\n",
    "        if not current_level_adj:\n",
    "             print(\"Current level is empty. Cannot create next level.\")\n",
    "             return None, None\n",
    "\n",
    "        nodes = list(current_level_adj.keys())\n",
    "        random.shuffle(nodes) # Process nodes in random order to avoid bias\n",
    "\n",
    "        supernode_counter = 0\n",
    "        new_adj = {} # Adjacency list for the next level\n",
    "        new_node_mapping = {} # Mapping from new supernodes to original nodes\n",
    "        node_to_supernode = {} # Maps nodes from current level to their new supernode ID\n",
    "        merged_nodes = set() # Track nodes already merged into a supernode\n",
    "\n",
    "        for node in nodes:\n",
    "            if node in merged_nodes:\n",
    "                continue # Skip node if already part of a supernode\n",
    "\n",
    "            # Start a new supernode with the current node\n",
    "            current_supernode_id = f\"S{current_level + 1}_{supernode_counter}\"\n",
    "            # Get original nodes represented by this current level 'node'\n",
    "            original_nodes_in_current = self.node_mapping[current_level].get(node, {node})\n",
    "            supernode_members = {node} # Nodes from current level in this new supernode\n",
    "            original_nodes_in_supernode = set(original_nodes_in_current) # Use set copy\n",
    "            node_to_supernode[node] = current_supernode_id\n",
    "            merged_nodes.add(node)\n",
    "\n",
    "            # Try to merge neighbors into this supernode\n",
    "            # Use a queue or list for neighbors to check for merging\n",
    "            candidates_to_merge = list(current_level_adj.get(node, set()))\n",
    "            processed_candidates = {node} # Track neighbors considered for merging with this supernode\n",
    "\n",
    "            while candidates_to_merge:\n",
    "                neighbor = candidates_to_merge.pop(0)\n",
    "\n",
    "                if neighbor in merged_nodes or neighbor in supernode_members:\n",
    "                    continue # Already merged or part of the current group\n",
    "\n",
    "                # Check merge constraints\n",
    "                should_merge = True\n",
    "                neighbor_original_nodes = self.node_mapping[current_level].get(neighbor, {neighbor})\n",
    "\n",
    "                # Constraint: Max original nodes per supernode\n",
    "                if max_nodes_per_supernode and \\\n",
    "                   (len(original_nodes_in_supernode) + len(neighbor_original_nodes)) > max_nodes_per_supernode:\n",
    "                    should_merge = False\n",
    "\n",
    "                # Add other constraints here if needed (e.g., based on edge weights, community structure)\n",
    "\n",
    "                if should_merge:\n",
    "                    # Merge neighbor into the current supernode\n",
    "                    supernode_members.add(neighbor)\n",
    "                    original_nodes_in_supernode.update(neighbor_original_nodes)\n",
    "                    node_to_supernode[neighbor] = current_supernode_id\n",
    "                    merged_nodes.add(neighbor)\n",
    "\n",
    "                    # Add neighbors of the merged node to the candidate list, if they aren't processed yet\n",
    "                    for next_neighbor in current_level_adj.get(neighbor, set()):\n",
    "                         if next_neighbor not in merged_nodes and next_neighbor not in processed_candidates:\n",
    "                             candidates_to_merge.append(next_neighbor)\n",
    "                             processed_candidates.add(next_neighbor) # Mark as considered for this merge group\n",
    "                # Mark this neighbor as processed for the current supernode attempt\n",
    "                processed_candidates.add(neighbor)\n",
    "\n",
    "\n",
    "            # Finalize the new supernode\n",
    "            if supernode_members: # Should always be true if loop started\n",
    "                new_adj[current_supernode_id] = set() # Initialize neighbors set\n",
    "                new_node_mapping[current_supernode_id] = original_nodes_in_supernode\n",
    "                supernode_counter += 1\n",
    "\n",
    "        # Check if any aggregation actually happened\n",
    "        if len(new_adj) == len(current_level_adj) and len(merged_nodes) == len(current_level_adj):\n",
    "            print(\"No significant aggregation occurred. Stopping hierarchy creation.\")\n",
    "            return None, None # Indicate no new level was effectively created\n",
    "\n",
    "        print(f\"Aggregated {len(current_level_adj)} nodes/supernodes into {len(new_adj)} new supernodes.\")\n",
    "\n",
    "        # Create edges between the new supernodes based on original connections\n",
    "        print(\"Creating edges between new supernodes...\")\n",
    "        edges_added_count = 0\n",
    "        for u_orig, neighbors in current_level_adj.items():\n",
    "            # Find the supernode containing u_orig (if any)\n",
    "            if u_orig not in node_to_supernode: continue\n",
    "            u_super = node_to_supernode[u_orig]\n",
    "\n",
    "            for v_orig in neighbors:\n",
    "                # Find the supernode containing v_orig (if any)\n",
    "                if v_orig not in node_to_supernode: continue\n",
    "                v_super = node_to_supernode[v_orig]\n",
    "\n",
    "                # If u_orig and v_orig are in different supernodes, add an edge between them\n",
    "                if u_super != v_super:\n",
    "                    # Add edge if not already present (using sets handles this)\n",
    "                    if v_super not in new_adj[u_super]:\n",
    "                         new_adj[u_super].add(v_super)\n",
    "                         new_adj[v_super].add(u_super) # Assuming undirected\n",
    "                         edges_added_count += 1\n",
    "\n",
    "        print(f\"Added {edges_added_count} edges between supernodes in Level {current_level + 1}.\")\n",
    "        return new_adj, new_node_mapping\n",
    "\n",
    "    def build_hierarchy(self, max_levels=5, max_nodes_per_supernode=None):\n",
    "        \"\"\"Builds multiple levels of the hierarchy.\"\"\"\n",
    "        print(f\"\\nBuilding Visual Sketch Hierarchy up to {max_levels} levels...\")\n",
    "        if max_nodes_per_supernode:\n",
    "            print(f\"Constraint: Max {max_nodes_per_supernode} original nodes per supernode.\")\n",
    "\n",
    "        while self.max_level < max_levels - 1:\n",
    "            current_adj = self.levels.get(self.max_level)\n",
    "            if not current_adj:\n",
    "                print(f\"Stopping: Level {self.max_level} is empty.\")\n",
    "                break\n",
    "\n",
    "            next_adj, next_mapping = self._create_next_level(\n",
    "                current_adj,\n",
    "                max_nodes_per_supernode=max_nodes_per_supernode\n",
    "            )\n",
    "\n",
    "            if next_adj is None:\n",
    "                # No effective aggregation happened, stop building levels\n",
    "                break\n",
    "\n",
    "            # Successfully created a new level\n",
    "            self.max_level += 1\n",
    "            self.levels[self.max_level] = next_adj\n",
    "            self.node_mapping[self.max_level] = next_mapping\n",
    "\n",
    "        print(f\"\\nHierarchy built up to Level {self.max_level}.\")\n",
    "\n",
    "    def get_level_view(self, level):\n",
    "        \"\"\"Returns the graph adjacency list at a specific level.\"\"\"\n",
    "        if level < 0 or level > self.max_level:\n",
    "             print(f\"Warning: Level {level} does not exist (max level is {self.max_level}).\")\n",
    "             return None\n",
    "        return self.levels.get(level)\n",
    "\n",
    "    def get_original_nodes(self, level, supernode_id):\n",
    "        \"\"\"Gets the set of original node IDs within a supernode at a given level.\"\"\"\n",
    "        if level < 0 or level > self.max_level:\n",
    "             print(f\"Warning: Level {level} does not exist.\")\n",
    "             return None\n",
    "        if level == 0:\n",
    "             # At level 0, the ID is the original node ID itself\n",
    "             # Check if it exists in the original mapping\n",
    "             return {supernode_id} if supernode_id in self.node_mapping[0] else None\n",
    "\n",
    "        # For levels > 0, look up the supernode ID in the mapping for that level\n",
    "        if level in self.node_mapping and supernode_id in self.node_mapping[level]:\n",
    "            return self.node_mapping[level][supernode_id]\n",
    "        else:\n",
    "             print(f\"Warning: Supernode ID '{supernode_id}' not found at level {level}.\")\n",
    "             return None\n",
    "\n",
    "    def get_memory_estimate(self):\n",
    "        total_mem = sys.getsizeof(self.levels) + sys.getsizeof(self.node_mapping) + sys.getsizeof(self.max_level)\n",
    "        # Estimate memory for levels dictionary\n",
    "        for level, adj in self.levels.items():\n",
    "            total_mem += sys.getsizeof(level) + sys.getsizeof(adj)\n",
    "            for node, neighbors in adj.items():\n",
    "                total_mem += sys.getsizeof(node) + sys.getsizeof(neighbors)\n",
    "                total_mem += sum(sys.getsizeof(n) for n in neighbors) # Memory for neighbors themselves\n",
    "\n",
    "        # Estimate memory for node_mapping dictionary\n",
    "        for level, mapping in self.node_mapping.items():\n",
    "            total_mem += sys.getsizeof(level) + sys.getsizeof(mapping)\n",
    "            for supernode, original_nodes in mapping.items():\n",
    "                total_mem += sys.getsizeof(supernode) + sys.getsizeof(original_nodes)\n",
    "                total_mem += sum(sys.getsizeof(n) for n in original_nodes) # Memory for original node IDs\n",
    "\n",
    "        return total_mem\n",
    "\n",
    "\n",
    "# --- END OF ORIGINAL CLASSES (with modifications) ---\n",
    "\n",
    "\n",
    "# --- Comparison Framework ---\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np # Needed for accuracy calculations\n",
    "\n",
    "def generate_graph(n, m_target, model='ba', seed=None):\n",
    "    \"\"\"Generates a NetworkX graph.\"\"\"\n",
    "    print(f\"\\nGenerating graph: N={n}, Target Edges≈{m_target}, Model='{model}'...\")\n",
    "    if model == 'ba': # Barabási-Albert model\n",
    "        # Parameter 'm' for BA is edges to attach from a new node.\n",
    "        # Target edges M ≈ n * m (roughly). So m ≈ M / n.\n",
    "        m_param = max(1, int(m_target / n))\n",
    "        G = nx.barabasi_albert_graph(n, m_param, seed=seed)\n",
    "        print(f\"Generated BA graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges (m_param={m_param}).\")\n",
    "    elif model == 'gnp': # Erdős-Rényi model\n",
    "        # Probability p = M / (N*(N-1)/2)\n",
    "        if n <= 1: p = 0.0\n",
    "        else: p = min(1.0, m_target / (n * (n - 1) / 2.0))\n",
    "        G = nx.gnp_random_graph(n, p, seed=seed)\n",
    "        print(f\"Generated GNP graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges (p={p:.4f}).\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown graph model: {model}\")\n",
    "\n",
    "    # Ensure graph is connected for certain tests (optional, might take time)\n",
    "    # if not nx.is_connected(G):\n",
    "    #     print(\"Graph is not connected. Trying to get largest component or add edges...\")\n",
    "    #     # Option 1: Use the largest connected component\n",
    "    #     largest_cc = max(nx.connected_components(G), key=len)\n",
    "    #     G = G.subgraph(largest_cc).copy()\n",
    "    #     print(f\"Using largest connected component: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n",
    "        # Option 2: Add edges to connect components (more complex)\n",
    "\n",
    "    # Add random weights for algorithms that use them\n",
    "    for u, v in G.edges():\n",
    "        G[u][v]['weight'] = random.uniform(1.0, 100.0)\n",
    "\n",
    "    return G\n",
    "\n",
    "def get_ground_truth(G):\n",
    "    \"\"\"Computes ground truth values for the graph G.\"\"\"\n",
    "    print(\"\\nCalculating ground truth values...\")\n",
    "    truth = {}\n",
    "    start_time = time.time()\n",
    "    truth['is_connected'] = nx.is_connected(G)\n",
    "    print(f\"  Connectivity: {truth['is_connected']} (in {time.time() - start_time:.4f}s)\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    if truth['is_connected']:\n",
    "         # Use networkx MST function which uses Kruskal/Prim depending on density\n",
    "         mst = nx.minimum_spanning_tree(G, weight='weight', algorithm='kruskal') # Or 'prim'\n",
    "         truth['mst_weight'] = mst.size(weight='weight')\n",
    "         truth['mst_edges'] = mst.number_of_edges()\n",
    "    else:\n",
    "         # For disconnected graphs, find MST weight of the forest\n",
    "         truth['mst_weight'] = 0\n",
    "         truth['mst_edges'] = 0\n",
    "         num_components = 0\n",
    "         for component_nodes in nx.connected_components(G):\n",
    "             subgraph = G.subgraph(component_nodes)\n",
    "             num_components += 1\n",
    "             if subgraph.number_of_nodes() > 0:\n",
    "                  mst_comp = nx.minimum_spanning_tree(subgraph, weight='weight')\n",
    "                  truth['mst_weight'] += mst_comp.size(weight='weight')\n",
    "                  truth['mst_edges'] += mst_comp.number_of_edges()\n",
    "         truth['num_components'] = num_components\n",
    "\n",
    "    print(f\"  MST Weight (or Forest): {truth['mst_weight']:.2f} ({truth['mst_edges']} edges) (in {time.time() - start_time:.4f}s)\")\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    truth['degrees'] = dict(G.degree())\n",
    "    print(f\"  Calculated {len(truth['degrees'])} node degrees (in {time.time() - start_time:.4f}s)\")\n",
    "\n",
    "    # Bipartiteness check (can be slow for large graphs)\n",
    "    # start_time = time.time()\n",
    "    # truth['is_bipartite'] = nx.is_bipartite(G)\n",
    "    # print(f\"  Bipartite check: {truth['is_bipartite']} (in {time.time() - start_time:.4f}s)\")\n",
    "\n",
    "\n",
    "    # Max weight matching is computationally expensive (NP-hard for general graphs)\n",
    "    # We can compute an approximation using nx.max_weight_matching for comparison\n",
    "    # Note: nx.max_weight_matching returns a set of edges (tuples)\n",
    "    # start_time = time.time()\n",
    "    # try:\n",
    "    #      # Set maxcardinality=False for max weight (True finds max cardinality matching)\n",
    "    #      # This can still be slow. Might skip for very large graphs.\n",
    "    #      approx_mwm = nx.max_weight_matching(G, maxcardinality=False, weight='weight')\n",
    "    #      truth['approx_mwm_weight'] = sum(G[u][v]['weight'] for u, v in approx_mwm)\n",
    "    #      truth['approx_mwm_size'] = len(approx_mwm)\n",
    "    #      print(f\"  Approx Max Weight Matching (nx): Weight={truth['approx_mwm_weight']:.2f}, Size={truth['approx_mwm_size']} (in {time.time() - start_time:.4f}s)\")\n",
    "    # except Exception as e:\n",
    "    #      print(f\"  Could not compute NetworkX max_weight_matching: {e}\")\n",
    "    #      truth['approx_mwm_weight'] = -1\n",
    "    #      truth['approx_mwm_size'] = -1\n",
    "\n",
    "\n",
    "    print(\"Ground truth calculation complete.\")\n",
    "    return truth\n",
    "\n",
    "def run_comparison(G, ground_truth):\n",
    "    \"\"\"Runs the implemented algorithms on graph G and compares results.\"\"\"\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    results = []\n",
    "\n",
    "    print(f\"\\n--- Running Comparisons on Graph (N={n}, M={m}) ---\")\n",
    "\n",
    "    # 1. GraphConnectivitySketch (L0 Sampling based)\n",
    "    print(\"\\nAlgorithm: GraphConnectivitySketch\")\n",
    "    algo_name = \"ConnectivitySketch (L0)\"\n",
    "    start_build = time.time()\n",
    "    conn_sketch = GraphConnectivitySketch(n)\n",
    "    for u, v in G.edges():\n",
    "        conn_sketch.add_edge(u, v)\n",
    "    build_time = time.time() - start_build\n",
    "    mem_estimate = conn_sketch.get_memory_estimate()\n",
    "\n",
    "    start_query = time.time()\n",
    "    # Run is_connected multiple times for average? No, single shot as per paper concept.\n",
    "    # The result is probabilistic. Let's run it a few times to see stability.\n",
    "    conn_results = [conn_sketch.is_connected() for _ in range(5)]\n",
    "    query_time = (time.time() - start_query) / 5\n",
    "    # Majority vote or report range? Let's take the most common result.\n",
    "    try:\n",
    "        from collections import Counter\n",
    "        conn_result = Counter(conn_results).most_common(1)[0][0]\n",
    "    except IndexError:\n",
    "        conn_result = None # Handle case where list is empty (e.g., n=0)\n",
    "\n",
    "    accuracy = (conn_result == ground_truth['is_connected']) if conn_result is not None else \"N/A\"\n",
    "    print(f\"  Build Time: {build_time:.4f}s\")\n",
    "    print(f\"  Query Time (is_connected avg 5 runs): {query_time:.6f}s\")\n",
    "    print(f\"  Memory Estimate: {mem_estimate / 1024:.2f} KB\")\n",
    "    print(f\"  Is Connected (Sketch): {conn_result} (vs Ground Truth: {ground_truth['is_connected']}) -> Accuracy: {accuracy}\")\n",
    "    results.append({\n",
    "        'Algorithm': algo_name, 'Build Time (s)': build_time, 'Query Time (s)': query_time,\n",
    "        'Memory (KB)': mem_estimate / 1024, 'Query': 'is_connected',\n",
    "        'Accuracy': 1 if accuracy == True else 0 if accuracy == False else 0.5, # Numeric accuracy\n",
    "        'Notes': f\"Probabilistic (Result: {conn_result})\"\n",
    "    })\n",
    "\n",
    "    # 2. GraphLinearMeasurements\n",
    "    print(\"\\nAlgorithm: GraphLinearMeasurements\")\n",
    "    algo_name = \"LinearMeasurements\"\n",
    "    # Parameter c controls m = c * n * log^2(n)\n",
    "    c_param = 4 # As used in the class default\n",
    "    start_build = time.time()\n",
    "    linear_sketch = GraphLinearMeasurements(n, c=c_param)\n",
    "    for u, v in G.edges():\n",
    "        linear_sketch.update(u, v, delta=1) # Assuming unweighted edges for simplicity\n",
    "    build_time = time.time() - start_build\n",
    "    mem_estimate = linear_sketch.get_memory_estimate()\n",
    "    print(f\"  Build Time: {build_time:.4f}s\")\n",
    "    print(f\"  Memory Estimate: {mem_estimate / 1024:.2f} KB (m={linear_sketch.m} measurements)\")\n",
    "\n",
    "    # --- Queries (Using Placeholders) ---\n",
    "    print(\"  Running queries (using PLACEHOLDER decoders)...\")\n",
    "    query_results = {}\n",
    "    query_times = {}\n",
    "\n",
    "    start_q = time.time()\n",
    "    query_results['is_connected'] = linear_sketch.is_connected() # Placeholder\n",
    "    query_times['is_connected'] = time.time() - start_q\n",
    "\n",
    "    start_q = time.time()\n",
    "    query_results['approx_mst_weight'] = linear_sketch.approx_mst_weight() # Placeholder\n",
    "    query_times['approx_mst_weight'] = time.time() - start_q\n",
    "\n",
    "    start_q = time.time()\n",
    "    query_results['is_bipartite'] = linear_sketch.is_bipartite() # Placeholder\n",
    "    query_times['is_bipartite'] = time.time() - start_q\n",
    "\n",
    "    print(f\"  Query Time (is_connected): {query_times['is_connected']:.6f}s -> Result: {query_results['is_connected']} (Placeholder!)\")\n",
    "    print(f\"  Query Time (approx_mst_w): {query_times['approx_mst_weight']:.6f}s -> Result: {query_results['approx_mst_weight']} (Placeholder!)\")\n",
    "    print(f\"  Query Time (is_bipartite): {query_times['is_bipartite']:.6f}s -> Result: {query_results['is_bipartite']} (Placeholder!)\")\n",
    "\n",
    "    # Record results, noting they are based on placeholders\n",
    "    results.append({\n",
    "        'Algorithm': algo_name, 'Build Time (s)': build_time, 'Query Time (s)': query_times['is_connected'],\n",
    "        'Memory (KB)': mem_estimate / 1024, 'Query': 'is_connected (placeholder)',\n",
    "        'Accuracy': 0.5, 'Notes': f\"Placeholder decoder (Result: {query_results['is_connected']})\"\n",
    "    })\n",
    "    results.append({\n",
    "        'Algorithm': algo_name, 'Build Time (s)': build_time, 'Query Time (s)': query_times['approx_mst_weight'],\n",
    "        'Memory (KB)': mem_estimate / 1024, 'Query': 'approx_mst_w (placeholder)',\n",
    "        'Accuracy': 0.5, 'Notes': f\"Placeholder decoder (Result: {query_results['approx_mst_weight']})\"\n",
    "    })\n",
    "\n",
    "    # 3. AdaptiveGraphSketch\n",
    "    print(\"\\nAlgorithm: AdaptiveGraphSketch\")\n",
    "    algo_name = \"AdaptiveSketch (Sparsify)\"\n",
    "    # Parameter gamma trades off sparsifier size/accuracy and time\n",
    "    gamma_param = 0.3 # Smaller gamma -> denser sparsifier -> slower but potentially more accurate\n",
    "    start_build = time.time()\n",
    "    adaptive_sketch = AdaptiveGraphSketch(n, gamma=gamma_param)\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        adaptive_sketch.stream_update(u, v, data.get('weight', 1.0))\n",
    "    build_time = time.time() - start_build # Time to ingest edges (doesn't include finalize)\n",
    "    mem_estimate_before_finalize = adaptive_sketch.get_memory_estimate()\n",
    "\n",
    "    # Finalize runs sparsification and computes queries (MST, Matching)\n",
    "    start_finalize = time.time()\n",
    "    # finalize() returns: mst_weight, matching_list, mst_time, matching_time\n",
    "    final_results = adaptive_sketch.finalize()\n",
    "    finalize_time = time.time() - start_finalize\n",
    "    mem_estimate_after_finalize = adaptive_sketch.get_memory_estimate() # Memory after sparsification\n",
    "\n",
    "    mst_weight_sparse = final_results[0]\n",
    "    matching_sparse = final_results[1]\n",
    "    mst_query_time = final_results[2] # Time for MST computation on sparsifier\n",
    "    match_query_time = final_results[3] # Time for Matching computation on sparsifier\n",
    "\n",
    "    mst_accuracy = 1.0 - abs(mst_weight_sparse - ground_truth['mst_weight']) / ground_truth['mst_weight'] if ground_truth['mst_weight'] > 0 else 1.0\n",
    "    # Matching accuracy is harder to quantify without ground truth Max Weight Matching\n",
    "\n",
    "    print(f\"  Build Time (Edge Ingestion): {build_time:.4f}s\")\n",
    "    print(f\"  Finalize Time (Sparsify + Queries): {finalize_time:.4f}s\")\n",
    "    print(f\"    Query Time (MST on sparsifier): {mst_query_time:.4f}s\")\n",
    "    print(f\"    Query Time (Matching on sparsifier): {match_query_time:.4f}s\")\n",
    "    print(f\"  Memory Estimate (After Sparsify): {mem_estimate_after_finalize / 1024:.2f} KB\")\n",
    "    print(f\"  MST Weight (Sparse): {mst_weight_sparse:.2f} (vs Ground Truth: {ground_truth['mst_weight']:.2f}) -> Accuracy: {mst_accuracy:.4f}\")\n",
    "    # print(f\"  Approx Max Weight Matching (Sparse): Size={len(matching_sparse)}, Weight={sum(w for _,_,w in matching_sparse):.2f}\")\n",
    "\n",
    "    results.append({\n",
    "        'Algorithm': algo_name, 'Build Time (s)': build_time + finalize_time, # Total time\n",
    "        'Query Time (s)': mst_query_time, # Just MST query time after finalize\n",
    "        'Memory (KB)': mem_estimate_after_finalize / 1024, 'Query': 'MST Weight (Exact on Sparse)',\n",
    "        'Accuracy': mst_accuracy, 'Notes': f\"Gamma={gamma_param}, Sparsifier Edges={adaptive_sketch.edge_count}\"\n",
    "    })\n",
    "    results.append({\n",
    "        'Algorithm': algo_name, 'Build Time (s)': build_time + finalize_time,\n",
    "        'Query Time (s)': match_query_time, # Matching query time\n",
    "        'Memory (KB)': mem_estimate_after_finalize / 1024, 'Query': 'Approx Max Weight Matching',\n",
    "        'Accuracy': 0.5, 'Notes': \"Accuracy difficult to measure here\"\n",
    "    })\n",
    "\n",
    "    # 4. ModifiedBoruvkaSketching (Fallback Implementation)\n",
    "    print(\"\\nAlgorithm: ModifiedBoruvkaSketching (Fallback)\")\n",
    "    algo_name = \"Boruvka (Fallback Impl)\"\n",
    "    start_build = time.time()\n",
    "    boruvka_sketch = ModifiedBoruvkaSketching(n)\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        boruvka_sketch.add_edge(u, v, data.get('weight', 1.0))\n",
    "    build_time = time.time() - start_build # Time to build edge heap\n",
    "    mem_estimate = boruvka_sketch.get_memory_estimate() # Memory for edge heap\n",
    "\n",
    "    start_query = time.time()\n",
    "    mst_edges_boruvka, mst_weight_boruvka = boruvka_sketch.find_spanning_tree()\n",
    "    query_time = time.time() - start_query # Time for MST computation\n",
    "\n",
    "    mst_accuracy = 1.0 - abs(mst_weight_boruvka - ground_truth['mst_weight']) / ground_truth['mst_weight'] if ground_truth['mst_weight'] > 0 else 1.0\n",
    "\n",
    "    print(f\"  Build Time (Add Edges): {build_time:.4f}s\")\n",
    "    print(f\"  Query Time (find_spanning_tree): {query_time:.4f}s\")\n",
    "    print(f\"  Memory Estimate (Edge List/Heap): {mem_estimate / 1024:.2f} KB\")\n",
    "    print(f\"  MST Weight (Boruvka): {mst_weight_boruvka:.2f} (vs Ground Truth: {ground_truth['mst_weight']:.2f}) -> Accuracy: {mst_accuracy:.4f}\")\n",
    "    results.append({\n",
    "        'Algorithm': algo_name, 'Build Time (s)': build_time,\n",
    "        'Query Time (s)': query_time,\n",
    "        'Memory (KB)': mem_estimate / 1024, 'Query': 'MST Weight (Exact)',\n",
    "        'Accuracy': mst_accuracy, 'Notes': f\"Fallback impl (not sketched)\"\n",
    "    })\n",
    "\n",
    "\n",
    "    # 5. gSketch (for Degree Estimation)\n",
    "    print(\"\\nAlgorithm: gSketch\")\n",
    "    algo_name = \"gSketch (Degree Est.)\"\n",
    "    # Parameters\n",
    "    num_partitions = 8 # More partitions -> more localized, maybe less contention\n",
    "    cms_width = 128    # Width of internal CMS\n",
    "    cms_depth = 5      # Depth of internal CMS\n",
    "    start_build = time.time()\n",
    "    # Provide node_ids upfront if known, helps partitioning consistency\n",
    "    gsketch_instance = gSketch(num_partitions, cms_width, cms_depth, node_ids=list(G.nodes()))\n",
    "    for u, v in G.edges():\n",
    "        gsketch_instance.add_edge(u, v, weight=1) # Use weight=1 for degree counting\n",
    "    build_time = time.time() - start_build\n",
    "    mem_estimate = gsketch_instance.get_memory_estimate()\n",
    "\n",
    "    # Query: Estimate degrees for a sample of nodes\n",
    "    query_time_total = 0\n",
    "    degree_errors = []\n",
    "    nodes_to_query = random.sample(list(G.nodes()), min(100, n)) # Query degree for 100 nodes or all if < 100\n",
    "    estimated_degrees = {}\n",
    "\n",
    "    start_query_all = time.time()\n",
    "    for node in nodes_to_query:\n",
    "        start_q_node = time.time()\n",
    "        est_degree = gsketch_instance.estimate_degree(node)\n",
    "        query_time_total += (time.time() - start_q_node)\n",
    "        estimated_degrees[node] = est_degree\n",
    "        true_degree = ground_truth['degrees'].get(node, 0)\n",
    "        degree_errors.append(abs(est_degree - true_degree))\n",
    "    query_time_avg = query_time_total / len(nodes_to_query) if nodes_to_query else 0\n",
    "\n",
    "    avg_abs_error = np.mean(degree_errors) if degree_errors else 0\n",
    "    # Relative error is tricky if true degree is 0. Use Mean Absolute Percentage Error (MAPE) carefully or just abs error.\n",
    "    # Accuracy score: 1 / (1 + avg_abs_error) -> ranges (0, 1], 1 is perfect.\n",
    "    accuracy_score = 1.0 / (1.0 + avg_abs_error)\n",
    "\n",
    "    print(f\"  Build Time: {build_time:.4f}s\")\n",
    "    print(f\"  Query Time (Estimate Degree avg over {len(nodes_to_query)} nodes): {query_time_avg:.8f}s\")\n",
    "    print(f\"  Memory Estimate: {mem_estimate / 1024:.2f} KB\")\n",
    "    print(f\"  Avg Absolute Degree Error: {avg_abs_error:.4f}\")\n",
    "    print(f\"  Degree Accuracy Score: {accuracy_score:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'Algorithm': algo_name, 'Build Time (s)': build_time,\n",
    "        'Query Time (s)': query_time_avg,\n",
    "        'Memory (KB)': mem_estimate / 1024, 'Query': 'Estimate Degree',\n",
    "        'Accuracy': accuracy_score, 'Notes': f\"AvgAbsErr={avg_abs_error:.2f}, Parts={num_partitions}, W={cms_width}, D={cms_depth}\"\n",
    "    })\n",
    "    # Store estimated degrees for potential plotting\n",
    "    results[-1]['details'] = {'estimated_degrees': estimated_degrees, 'nodes_queried': nodes_to_query}\n",
    "\n",
    "\n",
    "    # 6. GraphVisualSketchHierarchy (Qualitative / Structural)\n",
    "    print(\"\\nAlgorithm: GraphVisualSketchHierarchy\")\n",
    "    algo_name = \"VisualSketchHierarchy\"\n",
    "    start_build = time.time()\n",
    "    # Convert nx graph to simple dict adjacency list\n",
    "    adj_dict = {node: set(neighbors) for node, neighbors in G.adjacency()}\n",
    "    hierarchy = GraphVisualSketchHierarchy(adj_dict)\n",
    "    # Build a few levels\n",
    "    hierarchy.build_hierarchy(max_levels=4, max_nodes_per_supernode=max(10, n // 20)) # Limit supernode size\n",
    "    build_time = time.time() - start_build\n",
    "    mem_estimate = hierarchy.get_memory_estimate()\n",
    "\n",
    "    # Query time is not typical; it's about retrieving level views\n",
    "    start_query = time.time()\n",
    "    level1_view = hierarchy.get_level_view(1)\n",
    "    query_time = time.time() - start_query # Time to get one level view\n",
    "\n",
    "    num_nodes_l0 = len(hierarchy.get_level_view(0))\n",
    "    num_nodes_l1 = len(level1_view) if level1_view else 0\n",
    "    reduction_factor = num_nodes_l1 / num_nodes_l0 if num_nodes_l0 > 0 else 0\n",
    "\n",
    "    print(f\"  Build Time (Hierarchy): {build_time:.4f}s\")\n",
    "    print(f\"  Query Time (Get Level 1 View): {query_time:.6f}s\")\n",
    "    print(f\"  Memory Estimate: {mem_estimate / 1024:.2f} KB\")\n",
    "    print(f\"  Level 0 Nodes: {num_nodes_l0}, Level 1 Nodes: {num_nodes_l1} (Reduction: {reduction_factor:.2f})\")\n",
    "\n",
    "    results.append({\n",
    "        'Algorithm': algo_name, 'Build Time (s)': build_time,\n",
    "        'Query Time (s)': query_time,\n",
    "        'Memory (KB)': mem_estimate / 1024, 'Query': 'Build Hierarchy / Get View',\n",
    "        'Accuracy': reduction_factor, # Use reduction factor as a proxy metric\n",
    "        'Notes': f\"Built {hierarchy.max_level+1} levels. L1 Nodes={num_nodes_l1}\"\n",
    "    })\n",
    "    # Store hierarchy object for potential visualization later\n",
    "    results[-1]['details'] = {'hierarchy_object': hierarchy}\n",
    "\n",
    "\n",
    "    print(\"\\n--- Comparison Run Complete ---\")\n",
    "    return results, ground_truth # Return results and GT for plotting\n",
    "\n",
    "# --- Visualization Functions ---\n",
    "\n",
    "def plot_performance(results_df):\n",
    "    \"\"\"Plots Build Time, Query Time, and Memory Usage.\"\"\"\n",
    "    print(\"\\n--- Generating Performance Plots ---\")\n",
    "    if results_df.empty:\n",
    "        print(\"No results to plot.\")\n",
    "        return\n",
    "\n",
    "    # Prepare data: Melt for easier plotting with seaborn\n",
    "    perf_df = results_df.melt(id_vars=['Algorithm', 'Query', 'Notes'],\n",
    "                              value_vars=['Build Time (s)', 'Query Time (s)', 'Memory (KB)'],\n",
    "                              var_name='Metric', value_name='Value')\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "    # Plot Build Time (Log scale often useful for time)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    build_data = perf_df[perf_df['Metric'] == 'Build Time (s)']\n",
    "    # Use unique build times per algorithm (some algos listed multiple times for different queries)\n",
    "    build_data_unique = build_data.drop_duplicates(subset=['Algorithm'])\n",
    "    sns.barplot(data=build_data_unique, x='Algorithm', y='Value', palette='viridis')\n",
    "    plt.title('Algorithm Build Time')\n",
    "    plt.ylabel('Time (s) - Log Scale')\n",
    "    plt.yscale('log') # Use log scale if times vary widely\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plot_build_time.png\")\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Plot Query Time (Log scale often useful) - Separate plot per query type might be clearer\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    query_data = perf_df[perf_df['Metric'] == 'Query Time (s)']\n",
    "    sns.barplot(data=query_data, x='Algorithm', y='Value', hue='Query', palette='magma', dodge=True)\n",
    "    plt.title('Algorithm Query Time (Lower is Better)')\n",
    "    plt.ylabel('Time (s) - Log Scale')\n",
    "    plt.yscale('log')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Query Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plot_query_time.png\")\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Memory Usage\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    mem_data = perf_df[perf_df['Metric'] == 'Memory (KB)']\n",
    "    # Use unique memory usage per algorithm instance\n",
    "    mem_data_unique = mem_data.drop_duplicates(subset=['Algorithm'])\n",
    "    sns.barplot(data=mem_data_unique, x='Algorithm', y='Value', palette='plasma')\n",
    "    plt.title('Estimated Memory Usage')\n",
    "    plt.ylabel('Memory (KB) - Log Scale')\n",
    "    plt.yscale('log') # Use log scale if memory varies widely\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plot_memory_usage.png\")\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    # Filter out placeholder results where accuracy is meaningless (0.5)\n",
    "    acc_data = results_df[results_df['Accuracy'] != 0.5]\n",
    "    if not acc_data.empty:\n",
    "        sns.barplot(data=acc_data, x='Algorithm', y='Accuracy', hue='Query', palette='coolwarm', dodge=True)\n",
    "        plt.title('Algorithm Accuracy (Higher is Better, Placeholders Excluded)')\n",
    "        plt.ylabel('Accuracy Score (0 to 1)')\n",
    "        plt.ylim(0, 1.1) # Set Y-axis limits\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Query Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"plot_accuracy.png\")\n",
    "        # plt.show()\n",
    "    else:\n",
    "        print(\"No non-placeholder accuracy results to plot.\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    print(\"Performance plots generated and saved.\")\n",
    "\n",
    "\n",
    "def plot_degree_estimation(results_df, ground_truth):\n",
    "    \"\"\"Plots gSketch degree estimation accuracy.\"\"\"\n",
    "    print(\"\\n--- Generating Degree Estimation Plot (gSketch) ---\")\n",
    "    gsketch_result = results_df[(results_df['Algorithm'] == 'gSketch (Degree Est.)') & ('details' in results_df.columns)].iloc[0]\n",
    "\n",
    "    if not gsketch_result.empty and 'details' in gsketch_result and gsketch_result['details']:\n",
    "        details = gsketch_result['details']\n",
    "        estimated_degrees = details.get('estimated_degrees', {})\n",
    "        nodes_queried = details.get('nodes_queried', [])\n",
    "\n",
    "        if not estimated_degrees or not nodes_queried:\n",
    "            print(\"No degree estimation data found for gSketch.\")\n",
    "            return\n",
    "\n",
    "        true_degrees = [ground_truth['degrees'].get(node, 0) for node in nodes_queried]\n",
    "        est_degrees_list = [estimated_degrees.get(node, 0) for node in nodes_queried]\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        sns.scatterplot(x=true_degrees, y=est_degrees_list, alpha=0.7)\n",
    "        # Add y=x line for reference\n",
    "        max_degree = max(max(true_degrees), max(est_degrees_list))\n",
    "        plt.plot([0, max_degree], [0, max_degree], color='red', linestyle='--', label='Perfect Estimation (y=x)')\n",
    "        plt.title('gSketch Degree Estimation Accuracy')\n",
    "        plt.xlabel('True Degree')\n",
    "        plt.ylabel('Estimated Degree (CMS)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"plot_gsketch_degree_accuracy.png\")\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        print(\"gSketch degree accuracy plot saved.\")\n",
    "    else:\n",
    "        print(\"Could not find detailed gSketch results for plotting.\")\n",
    "\n",
    "\n",
    "def plot_graph_visualizations(G, results_df):\n",
    "    \"\"\"Visualizes original graph vs. sparsified/hierarchy.\"\"\"\n",
    "    print(\"\\n--- Generating Graph Structure Visualizations ---\")\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    # 1. Original Graph (Sampled nodes/edges if too large)\n",
    "    plt.subplot(1, 3, 1)\n",
    "    pos = nx.spring_layout(G, seed=42, k=0.1) # Layout can be slow\n",
    "    node_sample = random.sample(list(G.nodes()), min(100, G.number_of_nodes()))\n",
    "    edge_sample = random.sample(list(G.edges()), min(200, G.number_of_edges()))\n",
    "    G_sample = G.edge_subgraph(edge_sample).copy() # Create subgraph from sampled edges\n",
    "    nodes_to_draw = list(G_sample.nodes()) # Draw nodes present in the edge sample\n",
    "    nx.draw_networkx_nodes(G_sample, pos, nodelist=nodes_to_draw, node_size=10, alpha=0.6)\n",
    "    nx.draw_networkx_edges(G_sample, pos, width=0.5, alpha=0.3)\n",
    "    plt.title(f\"Original Graph (Sample: {len(nodes_to_draw)}N, {len(edge_sample)}E)\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # 2. Adaptive Sketch Sparsifier\n",
    "    plt.subplot(1, 3, 2)\n",
    "    adaptive_result = results_df[(results_df['Algorithm'] == 'AdaptiveSketch (Sparsify)') & (results_df['Query'] == 'MST Weight (Exact on Sparse)')]\n",
    "    if not adaptive_result.empty:\n",
    "         # Need the actual sparsified graph object. This requires modifying the run_comparison\n",
    "         # or AdaptiveGraphSketch class to return/store the final graph.\n",
    "         # Let's assume we can reconstruct it from the _get_edges_from_sparsifier logic (hacky)\n",
    "         # This is inefficient and requires re-running part of the logic or storing the graph.\n",
    "         # For demo, we'll just show fewer edges conceptually.\n",
    "         # Proper way: Modify AdaptiveGraphSketch to store/return self.current_graph after finalize.\n",
    "         # Hacky Demo: Draw original graph pos, but with fewer edges shown\n",
    "         num_sparse_edges = int(adaptive_result['Notes'].iloc[0].split(\"=\")[-1]) if 'Edges=' in adaptive_result['Notes'].iloc[0] else len(edge_sample)//2\n",
    "         sparse_edge_sample = random.sample(edge_sample, min(len(edge_sample), num_sparse_edges))\n",
    "         G_sparse_sample = G.edge_subgraph(sparse_edge_sample).copy()\n",
    "         nodes_to_draw_sparse = list(G_sparse_sample.nodes())\n",
    "         nx.draw_networkx_nodes(G_sparse_sample, pos, nodelist=nodes_to_draw_sparse, node_size=10, alpha=0.6, node_color='orange')\n",
    "         nx.draw_networkx_edges(G_sparse_sample, pos, width=0.5, alpha=0.5, edge_color='orange')\n",
    "         plt.title(f\"Sparsifier (Conceptual, {num_sparse_edges} edges)\")\n",
    "         plt.axis('off')\n",
    "    else:\n",
    "         plt.title(\"Sparsifier (N/A)\")\n",
    "         plt.axis('off')\n",
    "\n",
    "\n",
    "    # 3. Visual Hierarchy Level 1\n",
    "    plt.subplot(1, 3, 3)\n",
    "    hierarchy_result = results_df[(results_df['Algorithm'] == 'VisualSketchHierarchy') & ('details' in results_df.columns)]\n",
    "    if not hierarchy_result.empty and 'details' in hierarchy_result.iloc[0] and hierarchy_result.iloc[0]['details']:\n",
    "        hierarchy = hierarchy_result.iloc[0]['details'].get('hierarchy_object')\n",
    "        if hierarchy:\n",
    "            level1_view = hierarchy.get_level_view(1)\n",
    "            if level1_view:\n",
    "                 # Create a NetworkX graph from the level 1 view (supernodes)\n",
    "                 G_level1 = nx.Graph()\n",
    "                 for snode, neighbors in level1_view.items():\n",
    "                     G_level1.add_node(snode)\n",
    "                     for neighbor_snode in neighbors:\n",
    "                         G_level1.add_edge(snode, neighbor_snode)\n",
    "\n",
    "                 pos_l1 = nx.spring_layout(G_level1, seed=42)\n",
    "                 nx.draw_networkx_nodes(G_level1, pos_l1, node_size=30, node_color='green', alpha=0.8)\n",
    "                 nx.draw_networkx_edges(G_level1, pos_l1, width=1.0, alpha=0.6, edge_color='green')\n",
    "                 plt.title(f\"Visual Hierarchy Level 1 ({G_level1.number_of_nodes()} Supernodes)\")\n",
    "                 plt.axis('off')\n",
    "            else:\n",
    "                 plt.title(\"Visual Hierarchy L1 (Empty)\")\n",
    "                 plt.axis('off')\n",
    "        else:\n",
    "            plt.title(\"Visual Hierarchy L1 (N/A)\")\n",
    "            plt.axis('off')\n",
    "    else:\n",
    "        plt.title(\"Visual Hierarchy L1 (N/A)\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plot_graph_visualizations.png\")\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "    print(\"Graph structure visualizations saved.\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    G = nx.read_edgelist('./dataset/twitch/DE/musae_DE_edges.csv', delimiter=',', nodetype=int, data=(('weight', float),))\n",
    "    G = nx.Graph(G)\n",
    "    # --- Calculate Ground Truth ---\n",
    "    ground_truth = get_ground_truth(G)\n",
    "\n",
    "    # --- Run Algorithm Comparison ---\n",
    "    comparison_results, _ = run_comparison(G, ground_truth)\n",
    "\n",
    "    # --- Print Summary Table ---\n",
    "    results_df = pd.DataFrame(comparison_results)\n",
    "    # Remove complex objects before printing\n",
    "    results_df_printable = results_df.drop(columns=['details'], errors='ignore')\n",
    "    print(\"\\n\\n--- Performance Summary ---\")\n",
    "    print(results_df_printable.to_string())\n",
    "\n",
    "    # --- Generate Plots ---\n",
    "    if not results_df.empty:\n",
    "         # Check if matplotlib is available\n",
    "         try:\n",
    "             import matplotlib.pyplot as plt\n",
    "             import seaborn as sns\n",
    "             plot_performance(results_df)\n",
    "             plot_degree_estimation(results_df, ground_truth) # Specific plot for gSketch\n",
    "             plot_graph_visualizations(G, results_df) # Structural plots\n",
    "         except ImportError:\n",
    "             print(\"\\n[WARN] Matplotlib or Seaborn not found. Skipping plot generation.\")\n",
    "             print(\"Please install them: pip install matplotlib seaborn\")\n",
    "    else:\n",
    "         print(\"\\nNo results data frame generated. Skipping plots.\")\n",
    "\n",
    "\n",
    "    # --- Textual Comparison & Contrast ---\n",
    "    print(\"\\n\\n--- Comparison and Contrast Summary ---\")\n",
    "    print(f\"Graph: N={G.number_of_nodes()}, M={G.number_of_edges()}, Connected={ground_truth.get('is_connected', 'N/A')}, MST Weight={ground_truth.get('mst_weight', 0):.2f}\")\n",
    "\n",
    "    # Find best performers for different metrics (example)\n",
    "    if not results_df.empty:\n",
    "        # Fastest build time (use unique algorithm build times)\n",
    "        build_times = results_df.drop_duplicates(subset=['Algorithm'])[['Algorithm', 'Build Time (s)']]\n",
    "        fastest_build = build_times.loc[build_times['Build Time (s)'].idxmin()]\n",
    "        print(f\"\\nFastest Build Time: {fastest_build['Algorithm']} ({fastest_build['Build Time (s)']:.4f}s)\")\n",
    "\n",
    "        # Lowest memory usage (use unique algorithm memory)\n",
    "        mem_usage = results_df.drop_duplicates(subset=['Algorithm'])[['Algorithm', 'Memory (KB)']]\n",
    "        lowest_mem = mem_usage.loc[mem_usage['Memory (KB)'].idxmin()]\n",
    "        print(f\"Lowest Memory Usage: {lowest_mem['Algorithm']} ({lowest_mem['Memory (KB)']:.2f} KB)\")\n",
    "\n",
    "        # Fastest query time for specific queries (e.g., Connectivity)\n",
    "        conn_query_times = results_df[results_df['Query'].str.contains(\"is_connected\", case=False, na=False)]\n",
    "        if not conn_query_times.empty:\n",
    "             fastest_conn_query = conn_query_times.loc[conn_query_times['Query Time (s)'].idxmin()]\n",
    "             print(f\"Fastest Connectivity Query: {fastest_conn_query['Algorithm']} ({fastest_conn_query['Query Time (s)']:.6f}s) - Note: May include placeholders/probabilistic.\")\n",
    "\n",
    "        # Highest accuracy for specific queries (e.g., MST)\n",
    "        mst_accuracy = results_df[results_df['Query'].str.contains(\"MST\", case=False, na=False) & (results_df['Accuracy'] != 0.5)] # Exclude placeholders\n",
    "        if not mst_accuracy.empty:\n",
    "             best_mst_acc = mst_accuracy.loc[mst_accuracy['Accuracy'].idxmax()]\n",
    "             print(f\"Highest MST Accuracy: {best_mst_acc['Algorithm']} ({best_mst_acc['Accuracy']:.4f})\")\n",
    "\n",
    "    print(\"\\nKey Observations & Trade-offs:\")\n",
    "    print(\"- L0-Sampling (ConnectivitySketch): Very fast updates and queries, low memory, but probabilistic connectivity results.\")\n",
    "    print(\"- LinearMeasurements: Fast updates, potentially higher memory (O(n log^2 n)). Query performance/accuracy depends heavily on the *decoding algorithms* (which were placeholders here). Theoretical power for various queries.\")\n",
    "    print(\"- AdaptiveSketch (Sparsification): Slower 'build' (ingestion+sparsification), but computes exact MST/approx Matching on the *smaller* sparsified graph. Accuracy depends on gamma and sparsifier quality. Memory usage drops after sparsification.\")\n",
    "    print(\"- Boruvka (Fallback): Used as a baseline here. Its performance is standard MST computation O(M log N or M log* N), not sketch-based. Memory stores all edges.\")\n",
    "    print(\"- gSketch: Partitioned approach. Fast updates. Very fast degree estimation queries. Accuracy depends on CMS parameters (width, depth) and partitioning. Good for localized queries.\")\n",
    "    print(\"- SimpleSetSketch: Not directly compared on graph-wide tasks here, but demonstrated low-level set operations (XOR properties). Very space-efficient for small sets. Useful building block.\")\n",
    "    print(\"- VisualHierarchy: Primarily for structural abstraction, not typical query performance. Build time depends on aggregation complexity. Memory stores multiple graph levels.\")\n",
    "\n",
    "    print(\"\\nChoosing an Algorithm:\")\n",
    "    print(\"- For **dynamic connectivity** on streams with low memory: L0-Sampling (ConnectivitySketch) is strong, accepting probabilistic results.\")\n",
    "    print(\"- For **static graph MST/Matching** where pre-processing is acceptable: AdaptiveSketch provides accuracy control via gamma, working on a smaller graph for queries.\")\n",
    "    print(\"- For **fast degree estimation** or other localized queries on streams: gSketch offers good performance through partitioning.\")\n",
    "    print(\"- For **diverse queries** on streams (if decoders implemented): LinearMeasurements offer theoretical breadth but require complex decoding.\")\n",
    "    print(\"- For **visual exploration** and multi-level analysis: VisualHierarchy.\")\n",
    "\n",
    "    print(\"\\nNote: Performance depends significantly on graph size, structure, implementation details, and chosen parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa5d35",
   "metadata": {},
   "source": [
    "## K-Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32925af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_edge_connectivity_basic(G, k):\n",
    "    \"\"\"\n",
    "    Basic algorithm to test if the graph G is at least k-edge-connected.\n",
    "    Returns True if yes, otherwise False.\n",
    "    \"\"\"\n",
    "    remaining_edges = set(G.edges())\n",
    "    all_forests = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Create a subgraph without the edges already used in previous forests\n",
    "        subG = nx.Graph()\n",
    "        subG.add_nodes_from(G.nodes())\n",
    "        subG.add_edges_from(remaining_edges)\n",
    "\n",
    "        # Find a spanning forest in subG\n",
    "        forest = nx.minimum_spanning_edges(subG, data=False)\n",
    "        Fi = set(forest)\n",
    "        all_forests.append(Fi)\n",
    "\n",
    "        # Remove edges of the found forest from the remaining edge set\n",
    "        remaining_edges -= Fi\n",
    "\n",
    "    # Union of all forests\n",
    "    union_forest = set().union(*all_forests)\n",
    "    H = nx.Graph()\n",
    "    H.add_nodes_from(G.nodes())\n",
    "    H.add_edges_from(union_forest)\n",
    "\n",
    "    # If H is not connected or is missing some cut, then it's not k-edge-connected\n",
    "    return nx.is_k_edge_connected(H, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33940758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sketch:\n",
    "    def __init__(self, G):\n",
    "        self.edges = set(G.edges())\n",
    "\n",
    "    def subtract(self, other):\n",
    "        self.edges -= other.edges\n",
    "\n",
    "    def get_spanning_forest(self):\n",
    "        tempG = nx.Graph()\n",
    "        tempG.add_edges_from(self.edges)\n",
    "        return Sketch.from_edges(nx.minimum_spanning_edges(tempG, data=False))\n",
    "\n",
    "    @classmethod\n",
    "    def from_edges(cls, edges):\n",
    "        s = cls(nx.Graph())  # empty graph\n",
    "        s.edges = set(edges)\n",
    "        return s\n",
    "\n",
    "\n",
    "def k_edge_connectivity_sketch(G, k):\n",
    "    \"\"\"\n",
    "    Sketch-emulated algorithm for testing k-edge-connectivity.\n",
    "    \"\"\"\n",
    "    sketches = [Sketch(G) for _ in range(k)]\n",
    "    forests = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Subtract previous forests from the current sketch\n",
    "        for j in range(i):\n",
    "            sketches[i].subtract(forests[j])\n",
    "\n",
    "        # Get new forest from the current sketch\n",
    "        Fi = sketches[i].get_spanning_forest()\n",
    "        forests.append(Fi)\n",
    "\n",
    "    # Combine all forests into a single graph and check k-edge-connectivity\n",
    "    final_graph = nx.Graph()\n",
    "    final_graph.add_nodes_from(G.nodes())\n",
    "    for forest in forests:\n",
    "        final_graph.add_edges_from(forest.edges)\n",
    "\n",
    "    return nx.is_k_edge_connected(final_graph, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d4c66fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class GraphSketch:\n",
    "    def __init__(self, num_nodes, dim=512, seed=None):\n",
    "        self.n = num_nodes\n",
    "        self.d = dim\n",
    "        self.sketch = np.zeros((self.n, self.d))\n",
    "        self.hashes = {}\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def _edge_to_vector(self, u, v):\n",
    "        \"\"\"\n",
    "        Returns a random vector representation for an edge (u, v).\n",
    "        Uses signed random projections.\n",
    "        \"\"\"\n",
    "        if (u, v) not in self.hashes and (v, u) not in self.hashes:\n",
    "            vec = self.rng.choice([-1, 1], size=self.d)\n",
    "            self.hashes[(u, v)] = vec\n",
    "            self.hashes[(v, u)] = -vec\n",
    "        return self.hashes[(u, v)]\n",
    "\n",
    "    def add_edge(self, u, v):\n",
    "        vec = self._edge_to_vector(u, v)\n",
    "        self.sketch[u] += vec\n",
    "        self.sketch[v] -= vec  # maintains flow conservation\n",
    "\n",
    "    def remove_edge(self, u, v):\n",
    "        vec = self._edge_to_vector(u, v)\n",
    "        self.sketch[u] -= vec\n",
    "        self.sketch[v] += vec\n",
    "\n",
    "    def add_graph(self, G):\n",
    "        for u, v in G.edges():\n",
    "            self.add_edge(u, v)\n",
    "\n",
    "    def subtract(self, other):\n",
    "        self.sketch -= other.sketch\n",
    "\n",
    "    def copy(self):\n",
    "        new = GraphSketch(self.n, self.d)\n",
    "        new.sketch = self.sketch.copy()\n",
    "        new.hashes = self.hashes\n",
    "        return new\n",
    "\n",
    "    def recover_spanning_forest(self, all_edges):\n",
    "        \"\"\"\n",
    "        Greedy sparse recovery: choose edges that help connect components and have significant sketch signal.\n",
    "        \"\"\"\n",
    "        forest = []\n",
    "        uf = UnionFind(self.n)\n",
    "        for u, v in sorted(all_edges, key=lambda e: -self.edge_score(e[0], e[1])):\n",
    "            if uf.find(u) != uf.find(v):\n",
    "                forest.append((u, v))\n",
    "                uf.union(u, v)\n",
    "        return forest\n",
    "\n",
    "    def edge_score(self, u, v):\n",
    "        \"\"\"\n",
    "        Computes a rough score of how much the edge (u, v) appears in the sketch.\n",
    "        Higher means more likely it's still in the sketch.\n",
    "        \"\"\"\n",
    "        vec = self._edge_to_vector(u, v)\n",
    "        score = np.dot(self.sketch[u], vec) - np.dot(self.sketch[v], vec)\n",
    "        return abs(score)\n",
    "\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "\n",
    "    def union(self, x, y):\n",
    "        xr = self.find(x)\n",
    "        yr = self.find(y)\n",
    "        if xr != yr:\n",
    "            self.parent[yr] = xr\n",
    "\n",
    "\n",
    "def k_edge_connectivity_sketch_real(G, k, dim=512):\n",
    "    n = G.number_of_nodes()\n",
    "    all_edges = list(G.edges())\n",
    "    sketches = [GraphSketch(n, dim, seed=i) for i in range(k)]\n",
    "\n",
    "    # Step 1: Create k independent sketches of G\n",
    "    for sketch in sketches:\n",
    "        sketch.add_graph(G)\n",
    "\n",
    "    forests = []\n",
    "    removed_edges = set()\n",
    "\n",
    "    for i in range(k):\n",
    "        # Subtract previous forests from sketch[i]\n",
    "        for j in range(i):\n",
    "            sketches[i].subtract(forests[j]['sketch'])\n",
    "\n",
    "        # Recover a forest\n",
    "        forest_edges = sketches[i].recover_spanning_forest(all_edges)\n",
    "\n",
    "        # Save the sketch of this forest\n",
    "        forest_sketch = GraphSketch(n, dim)\n",
    "        for u, v in forest_edges:\n",
    "            forest_sketch.add_edge(u, v)\n",
    "\n",
    "        forests.append({'edges': forest_edges, 'sketch': forest_sketch})\n",
    "        removed_edges.update(forest_edges)\n",
    "\n",
    "    # Combine all edges from forests\n",
    "    combined_graph = nx.Graph()\n",
    "    combined_graph.add_nodes_from(G.nodes())\n",
    "    for f in forests:\n",
    "        combined_graph.add_edges_from(f['edges'])\n",
    "\n",
    "    return nx.is_k_edge_connected(combined_graph, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "794d5ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "k=1 connected: True\n",
      "k=2 connected: False\n",
      "k=3 connected: False\n",
      "k=4 connected: False\n",
      "k=5 connected: False\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "G = nx.complete_graph(5)  # A 4-edge-connected graph\n",
    "print(k_edge_connectivity_basic(G, 3))       # Should return True\n",
    "print(k_edge_connectivity_sketch(G, 3))      # Should return True (simulated)\n",
    "# Create a 5-node complete graph (which is 4-edge-connected)\n",
    "G = nx.complete_graph(5)\n",
    "print(\"k=1 connected:\", k_edge_connectivity_sketch_real(G, 1))\n",
    "print(\"k=2 connected:\", k_edge_connectivity_sketch_real(G, 2))\n",
    "print(\"k=3 connected:\", k_edge_connectivity_sketch_real(G, 3))\n",
    "print(\"k=4 connected:\", k_edge_connectivity_sketch_real(G, 4))\n",
    "print(\"k=5 connected:\", k_edge_connectivity_sketch_real(G, 5))  # Should be False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8509b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "\n",
    "def measure_algorithm(algorithm_fn, G, k):\n",
    "    \"\"\"\n",
    "    Measure execution time and memory usage of an algorithm.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((algorithm_fn, (G, k)), max_usage=True)\n",
    "    end_time = time.time()\n",
    "\n",
    "    result = algorithm_fn(G, k)\n",
    "    return {\n",
    "        'result': result,\n",
    "        'time': end_time - start_time,\n",
    "        'memory': mem_usage\n",
    "    }\n",
    "\n",
    "def generate_or_load_graph(dataset_name='karate'):\n",
    "    \"\"\"\n",
    "    Load a graph dataset or generate one.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'karate':\n",
    "        return nx.karate_club_graph()\n",
    "    elif dataset_name == 'erdos_renyi':\n",
    "        return nx.erdos_renyi_graph(n=100, p=0.1)\n",
    "    elif dataset_name == 'complete':\n",
    "        return nx.complete_graph(20)\n",
    "    elif dataset_name == 'grid':\n",
    "        return nx.grid_2d_graph(10, 10)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset\")\n",
    "\n",
    "def benchmark_algorithms(graph_name, algorithms, k_values):\n",
    "    \"\"\"\n",
    "    Runs each algorithm on a dataset graph for multiple k-values.\n",
    "    \"\"\"\n",
    "    G = generate_or_load_graph(graph_name)\n",
    "    results = []\n",
    "\n",
    "    for k in k_values:\n",
    "        for name, algo in algorithms.items():\n",
    "            print(f\"Running {name} on {graph_name} (k={k})...\")\n",
    "            metrics = measure_algorithm(algo, G, k)\n",
    "            results.append({\n",
    "                'graph': graph_name,\n",
    "                'k': k,\n",
    "                'algorithm': name,\n",
    "                'result': metrics['result'],\n",
    "                'time (s)': round(metrics['time'], 4),\n",
    "                'memory (MB)': round(metrics['memory'], 4)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb42e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Basic on 'twitch' (k=1)...\n",
      "Running Sketch on 'twitch' (k=1)...\n",
      "Running Basic_Sketch on 'twitch' (k=1)...\n",
      "Running Basic on 'twitch' (k=2)...\n",
      "Running Sketch on 'twitch' (k=2)...\n",
      "Running Basic_Sketch on 'twitch' (k=2)...\n",
      "Running Basic on 'twitch' (k=3)...\n",
      "Running Sketch on 'twitch' (k=3)...\n",
      "Running Basic_Sketch on 'twitch' (k=3)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>graph</th>\n",
       "      <th>k</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>result</th>\n",
       "      <th>time (s)</th>\n",
       "      <th>memory (MB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>twitch</td>\n",
       "      <td>1</td>\n",
       "      <td>Basic</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7743</td>\n",
       "      <td>240.0820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>twitch</td>\n",
       "      <td>1</td>\n",
       "      <td>Sketch</td>\n",
       "      <td>True</td>\n",
       "      <td>3.2922</td>\n",
       "      <td>1642.1719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>twitch</td>\n",
       "      <td>1</td>\n",
       "      <td>Basic_Sketch</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7571</td>\n",
       "      <td>1572.2930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>twitch</td>\n",
       "      <td>2</td>\n",
       "      <td>Basic</td>\n",
       "      <td>False</td>\n",
       "      <td>1.2761</td>\n",
       "      <td>1566.2344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>twitch</td>\n",
       "      <td>2</td>\n",
       "      <td>Sketch</td>\n",
       "      <td>False</td>\n",
       "      <td>6.7086</td>\n",
       "      <td>3056.1602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>twitch</td>\n",
       "      <td>2</td>\n",
       "      <td>Basic_Sketch</td>\n",
       "      <td>False</td>\n",
       "      <td>1.3903</td>\n",
       "      <td>2989.2656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>twitch</td>\n",
       "      <td>3</td>\n",
       "      <td>Basic</td>\n",
       "      <td>False</td>\n",
       "      <td>1.7995</td>\n",
       "      <td>2979.6758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>twitch</td>\n",
       "      <td>3</td>\n",
       "      <td>Sketch</td>\n",
       "      <td>False</td>\n",
       "      <td>10.2281</td>\n",
       "      <td>4476.5898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>twitch</td>\n",
       "      <td>3</td>\n",
       "      <td>Basic_Sketch</td>\n",
       "      <td>False</td>\n",
       "      <td>2.1010</td>\n",
       "      <td>4335.4727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    graph  k     algorithm  result  time (s)  memory (MB)\n",
       "0  twitch  1         Basic    True    0.7743     240.0820\n",
       "1  twitch  1        Sketch    True    3.2922    1642.1719\n",
       "2  twitch  1  Basic_Sketch    True    0.7571    1572.2930\n",
       "3  twitch  2         Basic   False    1.2761    1566.2344\n",
       "4  twitch  2        Sketch   False    6.7086    3056.1602\n",
       "5  twitch  2  Basic_Sketch   False    1.3903    2989.2656\n",
       "6  twitch  3         Basic   False    1.7995    2979.6758\n",
       "7  twitch  3        Sketch   False   10.2281    4476.5898\n",
       "8  twitch  3  Basic_Sketch   False    2.1010    4335.4727"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithms = {\n",
    "    'Basic': k_edge_connectivity_basic,\n",
    "    'Sketch': k_edge_connectivity_sketch_real,\n",
    "    'Basic_Sketch': k_edge_connectivity_sketch\n",
    "}\n",
    "\n",
    "k_values = [1, 2, 3]\n",
    "\n",
    "\n",
    "G = nx.read_edgelist('./dataset/twitch/DE/musae_DE_edges.csv', delimiter=',', nodetype=int, data=(('weight', float),))\n",
    "G = nx.Graph(G)\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    for name, algo in algorithms.items():\n",
    "        print(f\"Running {name} on 'twitch' (k={k})...\")\n",
    "        metrics = measure_algorithm(algo, G, k)\n",
    "        results.append({\n",
    "            'graph': 'twitch',\n",
    "            'k': k,\n",
    "            'algorithm': name,\n",
    "            'result': metrics['result'],\n",
    "            'time (s)': round(metrics['time'], 4),\n",
    "            'memory (MB)': round(metrics['memory'], 4)\n",
    "        })\n",
    "                \n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f2cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
